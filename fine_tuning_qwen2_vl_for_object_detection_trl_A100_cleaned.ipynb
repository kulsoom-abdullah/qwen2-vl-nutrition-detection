{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKadZFQ2IdJb"
   },
   "source": [
    "# Fine-Tuning Qwen2-VL-7B for Nutrition Table Detection\n",
    "\n",
    "Fine-tune Qwen2-VL (a Vision-Language Model (VLM)) to detect nutrition tables in product images, starting from a zero-shot baseline and ending with LoRA-based experiments.\n",
    "\n",
    "**Project Abstract:** *This notebook documents the end-to-end process of fine-tuning the Qwen2-VL-7B model for nutrition table detection. Starting from a strong zero-shot baseline (0.590 Mean IoU), I systematically explored three QLoRA fine-tuning strategies, overcoming significant memory and hardware challenges. The best model achieved a **Mean IoU of 0.771**, a **30.7% relative improvement**, demonstrating the effectiveness of parameter-efficient fine-tuning for specialized vision-language tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ84pmL57JGf"
   },
   "source": [
    "## \ud83d\udccb Table of Contents\n",
    "\n",
    "1.  [Introduction & Motivation](#intro)\n",
    "2.  [Environment & Setup](#setup)\n",
    "3.  [Dataset Overview & Visualization](#dataset)\n",
    "4.  [Understanding the Qwen2-VL Model](#model)\n",
    "5.  [Zero-Shot Baseline Evaluation](#zeroshot)\n",
    "6.  [Fine-Tuning Strategy and Data Preparation](#strategy)\n",
    "7.  [Rationale for Parameter-Efficient Fine-Tuning (PEFT)](#peft)\n",
    "8.  [Fine-Tuning Experiments and Training](#training)\n",
    "9.  [Checkpoint Evaluation](#evaluation)\n",
    "10. [Final Results and Analysis](#results)\n",
    "11. [Production Deployment: Merging LoRA Adapters](#deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdc7yvCQ7JGf"
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction & Motivation\n",
    "\n",
    "In this notebook, I fine-tune [Qwen2-VL-7B](https://qwenlm.github.io/blog/qwen2-vl/) for detecting nutrition tables from product images from [Hugging Face](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection).\n",
    "\n",
    "If you are new to this kind of work, check out Daniel Godo's book. [A Hands-On Guide to Fine-Tuning Large\n",
    "Language Models with PyTorch and Hugging Face](https://leanpub.com/finetuning) by [Daniel Voigt Godoy](https://dvgodoy.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JATmSI8mcyW2"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Environment & Setup\n",
    "\n",
    " #### Environment\n",
    " - Hardware: NVIDIA A100 (40 GB)\n",
    " - Install dependencies: `pip install -r requirements.txt`\n",
    " - add env to the ipython kernel list\n",
    " - Hugging Face access: `huggingface-cli login` or set `HUGGINGFACE_HUB_TOKEN`\n",
    " - NOTE Colab uses: run !pip install in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKOJNiTTGsoi"
   },
   "outputs": [],
   "source": [
    "  #!pip install torch>=2.1 torchvision>=0.16 torchaudio>=2.1 \\\n",
    "  #              numpy pillow datasets>=2.20 transformers>=4.42 \\\n",
    "  #              accelerate>=0.27 trl>=0.9 peft>=0.12 safetensors>=0.4 \\\n",
    "  #              huggingface_hub>=0.23 tqdm matplotlib \\\n",
    "  #              bitsandbytes>=0.43 \\\n",
    "  #              qwen-vl-utils \\\n",
    "  #              seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uYhorebGYIH"
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw, Image, ImageFont\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    Qwen2VLForConditionalGeneration,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from huggingface_hub import login\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from qwen_vl_utils import process_vision_info, vision_process\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXob4iN6KJY_",
    "outputId": "e4ca533a-2f02-484b-dd65-5df7dd5ea2ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda, \"| CUDA avail:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upZjD4bH7JGh"
   },
   "source": [
    "#### Optional Settings for an Improved Jupyter Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "cHS9CyWTGYII",
    "outputId": "de339a11-c4f8-4e42-ed0e-49ee4b89e580"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, set_matplotlib_formats\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Only show explicitly plt.show() outputs\n",
    "%matplotlib inline\n",
    "plt.ioff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0-2Lso6wkIh"
   },
   "outputs": [],
   "source": [
    "login(token=\"YOUR_HF_TOKEN_HERE\")\n",
    "# !pip install hf_transfer\n",
    "# login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcL4-bwGIoaR",
    "outputId": "cf945599-515e-49b8-b453-a51e4335549f"
   },
   "source": [
    "\n",
    "## Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2BTm9Ug7JGi"
   },
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    # clear the current variables and clean the GPU to free up resources.\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlHOYyL4GYIJ"
   },
   "outputs": [],
   "source": [
    "def parse_bounding_boxes(response_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Parses a model's text response to find bounding box coordinates.\n",
    "    - Flexibly finds all numbers (int or float) in the text.\n",
    "    - Groups them into bounding boxes of 4.\n",
    "    - Converts them from the Qwen 0-1000 scale to a normalized [0, 1] scale.\n",
    "    - Returns a list of lists, with each inner list being [x_min, y_min, x_max, y_max].\n",
    "    - It intelligently sorts the coordinates to ensure (x_min, y_min) is the top-left corner.\n",
    "    \"\"\"\n",
    "    all_numbers_str = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', response_text)\n",
    "    if len(all_numbers_str) < 4:\n",
    "        return []\n",
    "\n",
    "    all_numbers = [float(n) for n in all_numbers_str]\n",
    "    num_boxes = len(all_numbers) // 4\n",
    "\n",
    "    parsed_boxes = []\n",
    "    for i in range(num_boxes):\n",
    "        start_index = i * 4\n",
    "        box_nums = all_numbers[start_index : start_index + 4]\n",
    "        c1, c2, c3, c4 = box_nums\n",
    "        x1, y1, x2, y2 = c1 / 1000.0, c2 / 1000.0, c3 / 1000.0, c4 / 1000.0\n",
    "\n",
    "        x_min = min(x1, x2)\n",
    "        y_min = min(y1, y2)\n",
    "        x_max = max(x1, x2)\n",
    "        y_max = max(y1, y2)\n",
    "\n",
    "        parsed_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    return parsed_boxes\n",
    "\n",
    "# Optional tests function and test cases\n",
    "def run_parser_test_suite():\n",
    "    \"\"\"\n",
    "    Tests the parse_bounding_boxes function against various text inputs.\n",
    "    The expected format is a list of lists: [[x_min, y_min, x_max, y_max], ...],\n",
    "    with all coordinates normalized to the [0, 1] range.\n",
    "    \"\"\"\n",
    "    test_cases = {\n",
    "        \"official_four_coords\": {\n",
    "            \"input\": \"I found two boxes. The first is at 0,12,0,35. The second is 0,67,0,85.\",\n",
    "            \"expected\": [[0.0, 0.012, 0.0, 0.035], [0.0, 0.067, 0.0, 0.085]]\n",
    "        },\n",
    "        \"official_two_pairs\": {\n",
    "            \"input\": \"The box is at (10, 20), (300, 400)\",\n",
    "            \"expected\": [[0.01, 0.02, 0.3, 0.4]]\n",
    "        },\n",
    "        \"three_boxes_two_pairs\": {\n",
    "            \"input\": \"(1,2),(3,4) and (5,6),(7,8) also (9,10),(11,12)\",\n",
    "            \"expected\": [[0.001, 0.002, 0.003, 0.004], [0.005, 0.006, 0.007, 0.008], [0.009, 0.01, 0.011, 0.012]]\n",
    "        },\n",
    "        # --- THIS TEST CASE IS NOW CORRECTED ---\n",
    "        \"brackets_float\": {\n",
    "            \"input\": \"[0.1, 0.2, 0.3, 0.4]\",\n",
    "            \"expected\": [[0.0001, 0.0002, 0.0003, 0.0004]] # One box from four numbers\n",
    "        },\n",
    "        \"conversational_text\": {\n",
    "            \"input\": \"I think the nutrition table is around 150, 200, 550, 750 on the label.\",\n",
    "            \"expected\": [[0.15, 0.2, 0.55, 0.75]]\n",
    "        },\n",
    "        \"desc w/ (int, int..)\": {\n",
    "            \"input\": \"bounding_box: (0, 0, 1000, 1000)\",\n",
    "            \"expected\": [[0.0, 0.0, 1.0, 1.0]]\n",
    "        },\n",
    "        \"no_box\": {\n",
    "            \"input\": \"There is no nutrition table in this image.\",\n",
    "            \"expected\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"--- Running Final Parser Test Suite ---\")\n",
    "    all_passed = True\n",
    "    for name, case in test_cases.items():\n",
    "        result = parse_bounding_boxes(case[\"input\"])\n",
    "        is_correct = True\n",
    "        if len(result) != len(case[\"expected\"]):\n",
    "            is_correct = False\n",
    "        else:\n",
    "            for res_box, exp_box in zip(result, case[\"expected\"]):\n",
    "                if not all(abs(r - e) < 1e-6 for r, e in zip(res_box, exp_box)):\n",
    "                    is_correct = False\n",
    "                    break\n",
    "\n",
    "        if is_correct:\n",
    "            print(f\"\u2705 PASSED: {name}\")\n",
    "        else:\n",
    "            print(f\"\u274c FAILED: {name} | Got: {result}, Expected: {case['expected']}\")\n",
    "            all_passed = False\n",
    "\n",
    "    if all_passed:\n",
    "        print(\"\\n\ud83c\udf89 All tests passed!\")\n",
    "\n",
    "\n",
    "# run_parser_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzjOSD9GGYIJ"
   },
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "  \"You are a vision-language model specializing in nutrition-table detection.\\n\"\n",
    "  \"Detect every nutrition table in the image and respond only with lines of the form:\\n\"\n",
    "  \"nutrition-table<box(x_min, y_min),(x_max, y_max)>\\n\"\n",
    "  \"Coordinates are integers between 0 and 1000 in a normalized coordinate system (x first, then y).\\n\"\n",
    "  \"If multiple tables exist, return each on a separate line. Do not extract or describe text.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = \"Detect all nutrition tables in this image and return the boxes.\"\n",
    "\n",
    "\n",
    "def run_inference(example_or_image, *, model=None, processor=None, prompt=None, max_new_tokens=128):\n",
    "  \"\"\"\n",
    "  Runs inference on a dataset example (raw or mapped) or a raw PIL image.\n",
    "  \"\"\"\n",
    "  mdl = model if model is not None else globals().get(\"model\")\n",
    "  proc = processor if processor is not None else globals().get(\"processor\")\n",
    "  if mdl is None or proc is None:\n",
    "      raise ValueError(\"Pass `model`/`processor`, or keep globals with those names available.\")\n",
    "\n",
    "  # Handle raw dicts with or without 'messages'\n",
    "  if isinstance(example_or_image, dict):\n",
    "      if \"messages\" in example_or_image:\n",
    "          messages = example_or_image[\"messages\"]\n",
    "          image = example_or_image[\"image\"]\n",
    "      else:\n",
    "          image = example_or_image[\"image\"]\n",
    "          messages = [\n",
    "              {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": [\n",
    "                      {\"type\": \"image\", \"image\": image},\n",
    "                      {\"type\": \"text\", \"text\": prompt or USER_PROMPT},\n",
    "                  ],\n",
    "              },\n",
    "          ]\n",
    "  else:\n",
    "      image = example_or_image\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"image\", \"image\": image},\n",
    "                  {\"type\": \"text\", \"text\": prompt or USER_PROMPT},\n",
    "              ],\n",
    "          },\n",
    "      ]\n",
    "\n",
    "  # Format messages\n",
    "  formatted_messages = []\n",
    "  for message in messages:\n",
    "      role = message.get(\"role\")\n",
    "      content = message.get(\"content\")\n",
    "\n",
    "      if isinstance(content, list) and content and isinstance(content[0], dict) and \"type\" in content[0]:\n",
    "          formatted_messages.append(message)\n",
    "          continue\n",
    "\n",
    "      text = content if isinstance(content, str) else \"\"\n",
    "      if role == \"user\":\n",
    "          formatted_messages.append({\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"image\", \"image\": image},\n",
    "                  {\"type\": \"text\", \"text\": text.replace(\"<|image_1|>\", \"\").strip()},\n",
    "              ],\n",
    "          })\n",
    "      else:\n",
    "          formatted_messages.append({\n",
    "              \"role\": role,\n",
    "              \"content\": [{\"type\": \"text\", \"text\": text}],\n",
    "          })\n",
    "\n",
    "  text_prompt = proc.tokenizer.apply_chat_template(\n",
    "      formatted_messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "  )\n",
    "\n",
    "  inputs = proc(\n",
    "      text=text_prompt,\n",
    "      images=[image],\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "  ).to(mdl.device)\n",
    "\n",
    "  if \"pixel_values\" in inputs:\n",
    "      inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(mdl.dtype)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      generated_ids = mdl.generate(\n",
    "          **inputs,\n",
    "          max_new_tokens=max_new_tokens,\n",
    "          do_sample=False,\n",
    "          num_beams=1,\n",
    "      )\n",
    "\n",
    "  trimmed_generated_ids = [\n",
    "      out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "\n",
    "  # Decode only the NEW tokens\n",
    "  response = proc.batch_decode(trimmed_generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9QXwbJ7ovM5"
   },
   "source": [
    "###  Dataset Loading & Exploration\n",
    "\n",
    "In this section, the [openfoodfacts/nutrition-table-detection](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection) dataset. This dataset contains product images, the extracted bar codes, and bounding boxes for the nutrition tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKIHSAHX7JGi"
   },
   "outputs": [],
   "source": [
    "# load the dataset into training and evaluation sets\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "dataset_train_raw = load_dataset(dataset_id, split=\"train\")\n",
    "dataset_test_raw = load_dataset(dataset_id, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "ktyu4N7AKJZB",
    "outputId": "dcb33feb-ff90-4a9e-eaf2-4ec2a0c1be26"
   },
   "outputs": [],
   "source": [
    "example =dataset_train_raw[657]\n",
    "pprint(example)\n",
    "# raw image (scaled down)\n",
    "_ = plt.figure(figsize=(6,6))\n",
    "_ = plt.imshow(example[\"image\"])\n",
    "_ = plt.axis(\"off\")\n",
    "_ = plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EmdNfKKJZB"
   },
   "source": [
    "<a id=\"dataset\"></a>\n",
    "## Dataset Overview & Visualization: Nutrition Table Detection\n",
    "\n",
    "This project utilizes the `openfoodfacts/nutrition-table-detection` dataset, which is available on Hugging Face. The dataset was created by **Open Food Facts** and was used to train their own production model for detecting nutrition tables, providing a robust, real-world foundation for this fine-tuning task.\n",
    "\n",
    "For our purposes, we will focus on the following key fields from each sample:\n",
    "\n",
    "* **`image`**: The input image loaded as a PIL object.\n",
    "* **`width` & `height`**: The original dimensions of the image in pixels. These are essential for visualizing the bounding boxes.\n",
    "* **`objects`**: A dictionary containing the ground-truth annotations for the image.\n",
    "    * **`bbox`**: A list containing the bounding box coordinates.\n",
    "    * **`category_name`**: A list containing the object's class name, the main one being `'nutrition-table'`.\n",
    "\n",
    "#### Normalized Bounding Box Coordinates\n",
    "\n",
    "The bounding box coordinates are **normalized**, meaning their values range from 0 to 1. The coordinates are provided in the format `[y_min, x_min, y_max, x_max]`.\n",
    "\n",
    "This is a standard practice in computer vision because it makes the model's training process independent of the input image's resolution. To properly visualize these normalized coordinates on an image, we must scale them back to pixel values using the image's original `width` and `height`:\n",
    "\n",
    "* `absolute_x = normalized_x * image_width`\n",
    "* `absolute_y = normalized_y * image_height`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "eQvNOB-57JGi",
    "outputId": "92761225-8f36-415a-c160-c05aedf4868a"
   },
   "outputs": [],
   "source": [
    "def show_bboxes(example, show_labels=True, figsize=(8, 8)):\n",
    "    \"\"\"\n",
    "    Show all bounding boxes for a single HF example dict from\n",
    "    openfoodfacts/nutrition-table-detection.\n",
    "\n",
    "    Args:\n",
    "        example: dict with keys [\"image\", \"objects\", \"image_id\", ...]\n",
    "        show_labels: draw 1..n in top-left inside each box\n",
    "        figsize: matplotlib figure size\n",
    "    \"\"\"\n",
    "    img = example[\"image\"].copy()\n",
    "    w, h = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # scale line width & font for visibility on big images\n",
    "    lw = max(2, h // 400)\n",
    "    fs = max(18, h // 30)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", fs)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for i, bb in enumerate(example[\"objects\"][\"bbox\"], start=1):\n",
    "        # dataset format: [y_min, x_min, y_max, x_max] normalized\n",
    "        y_min, x_min, y_max, x_max = map(float, bb)\n",
    "        x0, y0 = int(x_min * w), int(y_min * h)\n",
    "        x1, y1 = int(x_max * w), int(y_max * h)\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=lw)\n",
    "        if show_labels:\n",
    "            draw.text((x0 + 5, y0 + 5), str(i), fill=\"red\", font=font)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(img)\n",
    "    title = f\"Image ID: {example.get('image_id', 'unknown')} \u2022 {len(example['objects']['bbox'])} boxes\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_bboxes(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3aiodRaGYIK"
   },
   "source": [
    "### Analysis of Data Distributions\n",
    "\n",
    "After visualizing the dataset, I drew several key conclusions that directly influenced my modeling and memory management strategy.\n",
    "\n",
    "#### Key Observations & Implications\n",
    "\n",
    "* **Variable Image Resolutions**: The histograms show a wide distribution of image widths and heights, with no single standard size. While the Qwen2-VL architecture is designed to handle variable resolutions by breaking images into patches, this variation presents a significant memory challenge. A very large image can result in a long sequence of visual tokens, drastically increasing the VRAM required for even a single sample (`batch_size=1`). This observation validated my decision to implement a `MAX_PIXELS` limit as a crucial memory optimization technique.\n",
    "\n",
    "* **Small Bounding Boxes**: The bounding boxes for nutrition tables are typically small relative to the overall image dimensions. This suggests that the model needs to be effective at identifying small features within a larger context.\n",
    "\n",
    "* **Handling Multiple Detections**: While most images in this dataset contain a single nutrition table, a robust evaluation plan must account for cases with multiple ground-truth boxes or multiple model predictions. My approach for calculating the Mean IoU will be to match each predicted box to the ground-truth box that has the highest overlap. This ensures a fair evaluation, even in complex scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "wlr2lSwM7JGi",
    "outputId": "3fc5bca9-4a68-4be9-beee-83abc0f96971"
   },
   "outputs": [],
   "source": [
    "### get the histogram of the image sizes\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_image_stats(ds, split_name):\n",
    "  widths, heights, bbox_counts, unique_categories, categories = [], [], [], [], []\n",
    "\n",
    "  for row in ds:\n",
    "      w, h = row[\"image\"].size\n",
    "      widths.append(w)\n",
    "      heights.append(h)\n",
    "\n",
    "      names = row[\"objects\"].get(\"category_name\") or [\"unknown\"]\n",
    "      bbox_counts.append(len(names))\n",
    "      unique_categories.append(len(set(names)))\n",
    "      categories.append(\", \".join(names))\n",
    "\n",
    "  return pd.DataFrame({\n",
    "      \"width\": widths,\n",
    "      \"height\": heights,\n",
    "      \"bbox_count\": bbox_counts,\n",
    "      \"unique_categories\": unique_categories,\n",
    "      \"category\": categories,\n",
    "      \"split\": split_name,\n",
    "  })\n",
    "\n",
    "df_train = build_image_stats(dataset_train_raw, \"train\")\n",
    "df_eval = build_image_stats(dataset_test_raw, \"eval\")\n",
    "stats_df = pd.concat([df_train, df_eval], axis=0)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "_ = sns.histplot(data=stats_df, x=\"width\", hue=\"split\", stat=\"density\", ax=axes[0], bins=30)\n",
    "_ = axes[0].set_title(\"Image Width\")\n",
    "_ = sns.histplot(data=stats_df, x=\"height\", hue=\"split\", stat=\"density\", ax=axes[1], bins=30)\n",
    "_ = axes[1].set_title(\"Image Height\")\n",
    "# sns.histplot(data=stats_df, x=\"bbox_count\", hue=\"split\", discrete=True, ax=axes[2])\n",
    "_ = sns.histplot(data=stats_df, x=\"bbox_count\", hue=\"split\", ax=axes[2])\n",
    "_ = axes[2].set_title(\"# Bounding Boxes per Image\")\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8sM7TNsKKJZB",
    "outputId": "b81f0c50-6c30-418d-8473-009f9400a96c"
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(8,4))\n",
    "# sns.countplot(data=stats_df, x=\"unique_categories\", hue=\"split\", discrete=True)\n",
    "_ = sns.countplot(data=stats_df, x=\"unique_categories\", hue=\"split\")\n",
    "_ = plt.title(\"Unique Categories per Image\")\n",
    "plt.show()\n",
    "\n",
    "_ = plt.figure(figsize=(10,4))\n",
    "_ = sns.countplot(data=stats_df, x=\"category\", order=stats_df[\"category\"].value_counts().index)\n",
    "_ = plt.xticks(rotation=45, ha=\"right\")\n",
    "_ = plt.title(\"Category Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nFWDveC7JGi"
   },
   "source": [
    "<a id=\"model\"></a>\n",
    "## Understanding the Qwen2-VL Model\n",
    "\n",
    "Before using the model, it's important to understand its core components and data requirements.\n",
    "\n",
    "* **Architecture**: The model consists of a **Vision Encoder** to process image patches, a **Large Language Model (LLM)** for text, and a **Cross-Attention Mechanism** that allows the LLM to \"see\" the visual information. It uses **2D Rotary Position Embeddings (RoPE)** in the vision encoder to effectively understand the spatial relationships between image patches.\n",
    "\n",
    "* **The Processor**: The Hugging Face `processor` is a critical utility that bundles all necessary preprocessing. It applies a chat template to structure the conversation, tokenizes the text, and performs \"patch-ification\" to convert images into a sequence of visual tokens.\n",
    "\n",
    "* **Expected Bounding Box Format**: A key detail from the official [Qwen-VL paper](https://arxiv.org/pdf/2308.12966.pdf) is that the model expects bounding box coordinates to be scaled to an integer grid of **1000x1000**. My data preparation pipeline handles the conversion from the dataset's normalized `[0, 1]` coordinates into the required format: `nutrition-table<box(x1, y1),(x2, y2)>`.\n",
    "\n",
    "### The Processor: A Unified Preprocessing Pipeline\n",
    "\n",
    "The Hugging Face `processor` for Qwen2-VL is a critical utility that bundles all necessary preprocessing steps. It's more than just a tokenizer; it's a complete data preparation tool.\n",
    "\n",
    "1.  **Chat Template Application**: The process begins with the chat template. When given a conversational input (e.g., a user prompt with text and images), the processor's `apply_chat_template` function formats it into a single, structured string. It inserts control tokens like `<|im_start|>user` to manage turns and uses `<img>...</img>` as placeholders for images.\n",
    "\n",
    "2.  **Vision Processing**: For each image, the processor calls an internal function similar to `process_vision_info`. This function performs several key operations:\n",
    "    * It resizes and normalizes the image to the expected dimensions and pixel value range.\n",
    "    * It performs **\"patch-ification,\"** dicing the image into a sequence of smaller, fixed-size patches. These patches are the visual equivalent of text tokens.\n",
    "    * The final output is a `pixel_values` tensor, ready for the Vision Encoder.\n",
    "\n",
    "3.  **Text Tokenization**: The formatted prompt string (with image placeholders) is passed to the text tokenizer, which converts it into numerical `input_ids`.\n",
    "\n",
    "By handling these steps, the `processor` outputs a dictionary containing the `input_ids`, `pixel_values`, and `attention_mask` needed to feed the model.\n",
    "\n",
    "### Model Architecture and Forward Pass\n",
    "\n",
    "The Qwen2-VL architecture is designed to fuse these two modalities:\n",
    "\n",
    "* The **Vision Encoder**, a Transformer-based network, processes the image patches to extract high-level visual features.\n",
    "* The **LLM** processes the text tokens.\n",
    "* A **Cross-Attention Mechanism** acts as the bridge, allowing the LLM to \"look at\" the relevant visual features from the encoder at each step of text generation.\n",
    "\n",
    "For a prompt with multiple images, such as `<img>img1.jpg</img>Describe this. Now look at <img>img2.jpg</img> and compare.`, the model processes each image's patches separately. It uses techniques like \"forbidden attention\" to ensure that when generating text about the first image, it doesn't \"see\" the features from the second, maintaining context.\n",
    "\n",
    "#### Positional Awareness: 2D RoPE\n",
    "\n",
    "A key innovation in modern Transformers, including Qwen2-VL's vision encoder, is the use of **2D Rotary Position Embedding (RoPE)**.\n",
    "\n",
    "* **What is it?** Traditional position embeddings add a vector to each token to give it a sense of its absolute location (e.g., \"this is patch #5\"). RoPE, however, is a more elegant solution that **rotates** each patch's embedding vector by an angle proportional to its (x, y) coordinates.\n",
    "\n",
    "* **Why is it better?** This rotational method inherently encodes the **relative positions** between patches directly into the self-attention calculation. The model doesn't just know *where* a patch is; it has a built-in, efficient way to understand how far apart patch A is from patch B, both horizontally and vertically. This is crucial for vision tasks, as it helps the model understand the spatial relationships that form objects and scenes without needing extra learnable parameters for position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470,
     "referenced_widgets": [
      "abba4c6c7d984eb7a6997ebebd833cfc",
      "4b348ca55f364b56ab51449d981a8eb1",
      "80e0e35747144e2facf9c729eddec72c",
      "bdc7b49f14f54577a2fcf21ab9274749",
      "0ae437176b1b4d42ab3fdf58a06f64b6",
      "8d34b1b9837a4f6e98a772a606391695",
      "87cf8436e312480aa3200e69c0b45b8d",
      "a3c063821b054885895238d2ec201140",
      "73dd63bd57284cd79210dfb6864b0190",
      "9bb2497ee873427ea5fc6f0f01d5d565",
      "7336dfafc7d04847ac02d2a82cf4772d",
      "43302829387f41ae914e4401bd50f964",
      "fdc6afd0b8df4bdea09cf5ff26ccc74e",
      "5b2ffdbc904c44f9841fd3142135a4f6",
      "9aa4b556d6014353aee47c9db6c43d55",
      "f01d42f115f74e6c9f183a372d0a09cc",
      "fac4b5e68f844cc693c0fafb0a4903c0",
      "db30636d68904fc594c06cba94d9d8af",
      "72a9becb21af484896e96d564ea8e4f1",
      "f234f21546c5428dba9e93d23455e2a6",
      "5368e44e88a64033b30c66f171b352ff",
      "efe8fa14cea84736ace3d772df2ca47f",
      "4411f15ae7474673a14cd89c3767f8ab",
      "b3d3039d429e4a8ab7cb13034ff30c1b",
      "81ab1ad48bfd4f11a2e956120ebf46cd",
      "ccd4fe7dd1334273949c700a5ff4ad37",
      "9fe26d3f74d54b45b0eb6e25b65e4a3d",
      "7ac82b3c2a994f99923889be0077a804",
      "59d1275f8ffc4212be068745efa87101",
      "c4a4e0f156df4dbb8f1617b70850617d",
      "d1d9e3c5425541e895cc4f416cff928e",
      "5f93e4d97a024587ba9b7e2be9f04bca",
      "983a8840f572438ca6001df336780a57",
      "f014107251a94287982d28d1f77e233c",
      "a3ea2b226f4144f5b1217b718817181e",
      "f3ba779bf4c14569aaefff4783f71836",
      "018c3722592841f78ba2f437bcb2c33e",
      "9e47e9796e174191ae449acfef7d6081",
      "30eaab8c678749f9b517b67cd77feb12",
      "5f53640f46a04426a515cdb1652cb7ef",
      "7e8c243ccf3240ccaf55d933f54574d4",
      "dbe1b01c44724c42a39e0cab8afee8df",
      "247a9f0f75ff4ac1ac01e90098948b0f",
      "c09dadec840d445c80324489d6e5bdc6",
      "7e6b677a8b6340249b6385800848671e",
      "e0bcfeb22f3044f28cdb273ca0dd727d",
      "fe328b171f4641c69dadc60b187e7c5f",
      "21ce853621d14276aee01d5d9fc78350",
      "3457283978bd4ab9a44c845f553b830b",
      "2d62e59a8b074e2581b6f0b0d8dc0309",
      "f169ede9cc0a4883b317ad88afb6ea04",
      "b1fe37c8c7bb42febd31ce60fb2d6b73",
      "7b0989573f7946faba81b032e79dc3a0",
      "76332a9c68d34af0aad6470ee00fe0f3",
      "51ab573e7a7541d18d28e678a958171d",
      "e95c1e303936496bb5c47c57a7d1ae10",
      "d707b6e8febc46439d81cbe966f3ed59",
      "298c925f2e574b388482540b979febfb",
      "7e03914429da4bddb1f92292d49435ab",
      "63a955d09dc1434892d9d0168ca95166",
      "957dc22a8ac6451ebdaeb98853623b6d",
      "e1ace1c3cc684f8f9809dc9ccc8d5aa9",
      "f61c83d380ed4f34a49378a63bd5d902",
      "3e05dbe2489e4566818bd93f95c0c568",
      "17a0393c5c014079b350088ef323f993",
      "49fb1f0c81dd48cc829d25a8998b32b5",
      "3c48f554a6aa4d02bb3c3855b8f2a9c7",
      "c83c198edab04469a8e53b919d65a5e0",
      "6c4f9892335a46e4b843ad756630a4bb",
      "d2f209db71cc4fe4b08e05554a047b75",
      "abca82c73d9c495e9292cd2ee549e274",
      "a0fb31b4bfd74eff80b11141580a0f83",
      "a16e135177e841959ad517e4bd16827c",
      "3a3473be80fe404095b48ab032029103",
      "e6d84164600a4fbf8920daa5df04bd9c",
      "6829b7521ec94c8f92d983509456ef30",
      "2864941257de4ee3b7db8a59ec4f1b10",
      "e8fc3125f9294bd6b153595fc813f486",
      "f327502a113b40839b10bef9113e8a8d",
      "72c670b661c34491a417c553153d4284",
      "d867011150b046a789961cdfe309dfaa",
      "a68196ab54354b4193f0aa5fcdaf0c27",
      "10ee5b24458c429e85cd9ba61ac10dcf",
      "799d478021d94a4692f89599b9d2be0e",
      "0c28ff7cf6884e72810d0ee3954b2323",
      "a135322043654df4afe7e16d52b50cc9",
      "7fda3ccc397048a3931b479776856729",
      "8dfa571b843341a7a7e7328d38487ee2",
      "57ad5e44d7fc4e6ca7e66c2b4a41e0ee",
      "62d4117d0eb74058842507036e82717e",
      "9e215698a67b4691af7b28ead826ff98",
      "f40879bcb3794e3ab2e697141fcf3216",
      "6f2ac77ae6e7424ba0e281026fa435e3",
      "272604c8815748fc8ac44178fd82bdb6",
      "668aaa9bec774abd85eddc25d40a29a3",
      "30dc190837324d22896cf121fba41b5a",
      "538fb7a8044d47d7afdb967eceba4c6a",
      "4ed311d6bd6949b58e1c81f353d7bfbf",
      "c20727b3b9774256a3f8f9b4f1b2c8f5",
      "04074c79feba46218d4678c221c4bc8f",
      "a1354b06401b408c914ef1615562ec3b",
      "13e4f78aa2964e418f37be570891a352",
      "6aa714f64f59425a9589e3433224e4d8",
      "ec435a4b778d411facd6f286bcc566ff",
      "934906aa3c5c4e5197ff8a6303bcb581",
      "3aa93ed7a06d4795bc4434da94b380a9",
      "9dd0977fe0e1465488d103e15366b143",
      "3027a1f520874921b0eddaefc9119dd2",
      "d4301f51e3f24eb3b47d4bec4456e161",
      "e897063c8d6a457dba80b1f2a07a9ac4",
      "1c243860af9849e6a45e1df6986b5fad",
      "f38630024ec64769b6c3ae72db239455",
      "66ff1db7b19242e884d42665610099ba",
      "876190f6aac14b769eee0d2cec708f52",
      "53c38fde7de94cd9a9f8b3640ad92197",
      "bcf9fcbeb74642ac8788f426845d234f",
      "1855a4d983ef4878a876c4a751887451",
      "24652e5531404d60905d37c8fd037d2b",
      "880061c960814fd180b426d214f70020",
      "a64bc241422f4981900454571e6d837c",
      "2eaf74cdf0a349a3ba58eb6c00375702",
      "b063380607764f05859528608ed7fc79",
      "8a02542b96e34687a8ff20c3306d750b",
      "f0d1d95475cb4c18856d1b881528e1b1",
      "8c0be1ff98624452ab37bda377987814",
      "37df6dfd002147fe86351b64413d31a2",
      "3ea77ae2feb8431e9af3fcbf74ade173",
      "e74afabea3a949cfaffeca0465358ab8",
      "e4b48f3f13ff4a028f895d778b5b311d",
      "b193e19c4eca457abea48a79b7dddcbd",
      "456c288da5024ec085e2972235115881",
      "d84c31759752464bb7e486a18b205e3b",
      "cb3fc861c20844a4b990d0da8428e8ba",
      "b99709685b464b4d854ad25cc0195491",
      "e82919e0b4e64421b9f761819244f855",
      "92aa622546fa48a782c4d8e0acf8e77e",
      "bd20ef9612294259a18c669d571a1140",
      "dd04f4a514b94e0f84968b0ee11902e6",
      "885df03db08845aeaf21608f2cb8002d",
      "0cc0fb463f0449cca95244936cfc3728",
      "ea016be859854295a05bbe42ce1ee198",
      "451e64b0de9f4747a842fa7ed688f40d",
      "588dc400613242a5998a9faaf7571c78"
     ]
    },
    "id": "vw_RG5kw7JGj",
    "outputId": "11b32755-8743-477d-dd29-6ce444de642c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\",\n",
    " bnb_4bit_compute_dtype=torch.bfloat16,\n",
    " bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    " \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    " quantization_config=bnb_config,\n",
    " device_map=\"auto\",\n",
    " trust_remote_code=True,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7T9cY92GYIK"
   },
   "source": [
    "### Baseline Model Memory Footprint\n",
    "\n",
    "Loading the base **Qwen2-VL-7B** model in its 16-bit format reveals its resource needs before any optimization.\n",
    "\n",
    "* **Parameters (4.7B)**: The model's weights require ~8.74 GB of VRAM.\n",
    "* **CUDA Allocated (9.02 GB)**: This is the active memory holding the model's weights.\n",
    "* **CUDA Reserved (13.73 GB)**: This is the total memory pool PyTorch has allocated from the GPU for current and future operations (like activations during inference).\n",
    "\n",
    "This initial ~14 GB footprint confirms that full fine-tuning is challenging even on high-end GPUs like the **A100 40GB**, making parameter-efficient techniques like **LoRA** essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxNqMRtFGYIK",
    "outputId": "44c09e18-7ff3-404f-f828-16b893d334c4"
   },
   "outputs": [],
   "source": [
    "def print_model_memory(model):\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  total_gb = total_params * 2 / 1024**3  # bfloat16 weights = 2 bytes\n",
    "  print(f\"Parameters: {total_params:,} (~{total_gb:.2f} GB)\")\n",
    "  print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      print(f\"CUDA memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "      print(f\"CUDA memory reserved:  {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "print_model_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O47vXxeyGYIK"
   },
   "outputs": [],
   "source": [
    "def evaluate_vlm(model, processor, dataset, max_samples=None, iou_threshold=0.5, max_new_tokens=128):\n",
    "  \"\"\"\n",
    "  Evaluates a vision-language model on object detection.\n",
    "  \n",
    "  Calculates:\n",
    "  1. True Mean IoU: Average of best IoU for each GT box (no threshold)\n",
    "     - Each GT box is matched to its best prediction\n",
    "     - Unmatched GT boxes contribute 0\n",
    "     - This is the TRUE mean across all GT boxes\n",
    "  \n",
    "  2. Threshold-based metrics (precision, recall, F1):\n",
    "     - Uses iou_threshold for counting TP/FP/FN\n",
    "     - Greedy matching above threshold\n",
    "  \n",
    "  Args:\n",
    "      model: VLM model\n",
    "      processor: Model processor\n",
    "      dataset: Test dataset (list or HF dataset)\n",
    "      max_samples: Optional limit on samples\n",
    "      iou_threshold: Threshold for precision/recall/F1 (NOT used for mean IoU)\n",
    "      max_new_tokens: Max tokens for generation\n",
    "  \n",
    "  Returns:\n",
    "      dict with mean_gt_iou, precision, recall, f1, samples_evaluated\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  total_iou_sum = 0.0\n",
    "  total_gt_boxes = 0\n",
    "  tp, fp, fn = 0, 0, 0\n",
    "\n",
    "  samples = dataset[:max_samples] if max_samples else dataset\n",
    "\n",
    "  for example in samples:\n",
    "      response = run_inference(\n",
    "          example,\n",
    "          model=model,\n",
    "          processor=processor,\n",
    "          max_new_tokens=max_new_tokens\n",
    "      )\n",
    "      pred_boxes = parse_bounding_boxes(response)\n",
    "      gt_boxes = example[\"objects\"][\"bbox\"]\n",
    "\n",
    "      # Increment total ground truth boxes\n",
    "      total_gt_boxes += len(gt_boxes)\n",
    "\n",
    "      if not pred_boxes or not gt_boxes:\n",
    "          if not pred_boxes:\n",
    "              fn += len(gt_boxes)  # Missed all GT boxes\n",
    "          if not gt_boxes:\n",
    "              fp += len(pred_boxes)  # All predictions are false positives\n",
    "          continue\n",
    "\n",
    "      pred_tensor = torch.tensor(pred_boxes, dtype=torch.float32)\n",
    "      gt_tensor = torch.tensor(gt_boxes, dtype=torch.float32)[:, [1, 0, 3, 2]]\n",
    "\n",
    "      iou_matrix = box_iou(pred_tensor, gt_tensor)  # [num_pred, num_gt]\n",
    "\n",
    "      # --- 1. True Mean IoU Calculation (No Threshold) ---\n",
    "      # For each GT box, find the IoU of its best-matching prediction.\n",
    "      # If a GT box has no match, its best IoU is 0.\n",
    "      if iou_matrix.numel() > 0:\n",
    "          best_ious_for_gt, _ = iou_matrix.max(dim=0)  # Best pred for each GT\n",
    "          total_iou_sum += best_ious_for_gt.sum().item()\n",
    "      # else: no predictions, all GTs contribute 0 (already counted in total_gt_boxes)\n",
    "\n",
    "      # --- 2. Precision/Recall/F1 Calculation (With Threshold) ---\n",
    "      # Use greedy matching to find true positives above threshold\n",
    "      all_pairs = sorted(\n",
    "          [(iou_matrix[p, g].item(), p, g)\n",
    "           for p in range(iou_matrix.shape[0])\n",
    "           for g in range(iou_matrix.shape[1])],\n",
    "          reverse=True\n",
    "      )\n",
    "\n",
    "      matched_preds = set()\n",
    "      matched_gts = set()\n",
    "\n",
    "      for iou, p, g in all_pairs:\n",
    "          if iou < iou_threshold:  # \u2190 Threshold ONLY affects TP/FP/FN\n",
    "              break\n",
    "          if p in matched_preds or g in matched_gts:\n",
    "              continue\n",
    "          matched_preds.add(p)\n",
    "          matched_gts.add(g)\n",
    "\n",
    "      tp += len(matched_preds)\n",
    "      fp += len(pred_boxes) - len(matched_preds)\n",
    "      fn += len(gt_boxes) - len(matched_preds)\n",
    "\n",
    "  # Final calculations\n",
    "  mean_iou = total_iou_sum / total_gt_boxes if total_gt_boxes else 0.0\n",
    "  precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "  recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "  f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "  return {\n",
    "      'mean_gt_iou': mean_iou,\n",
    "      f'precision@{iou_threshold:.2f}': precision,\n",
    "      f'recall@{iou_threshold:.2f}': recall,\n",
    "      f'f1@{iou_threshold:.2f}': f1,\n",
    "      'samples_evaluated': len(samples),\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BR19qpWGYIK"
   },
   "source": [
    "<a id=\"zeroshot\"></a>\n",
    "## Zero-Shot Baseline Evaluation\n",
    "\n",
    "My initial tests with a simple prompt confirmed the model's default behavior is to perform **Optical Character Recognition (OCR)**. To get a true detection baseline, I had to engineer a more effective prompt to override this behavior.\n",
    "\n",
    "#### Crafting the Final Prompt\n",
    "\n",
    "The final prompt was designed to be highly explicit, aligning with the model's training data:\n",
    "1.  **It defines the task** (\"Detect all...\").\n",
    "2.  **It specifies the exact output format** (`\"nutrition_label<box...>\"`) and **coordinate system** (\"...on a 1000x1000 canvas\").\n",
    "3.  **It includes a negative constraint** to prevent OCR (\"Do not extract or describe any text...\").\n",
    "\n",
    "#### Final Baseline Results\n",
    "\n",
    "Using this engineered prompt, I ran the evaluation on the entire test set of 123 samples to get the final, official baseline metrics.\n",
    "\n",
    "* **Mean IoU**: **0.27**\n",
    "* **F1-Score (@0.50 IoU)**: **0.386**\n",
    "* **Precision (@0.50 IoU)**: 0.395\n",
    "* **Recall (@0.50 IoU)**: 0.377\n",
    "\n",
    "This proves that while the model can be guided to understand the task, it lacks the specialized ability to perform it accurately, justifying the need for fine-tuning.\n",
    "\n",
    "Before fine-tuning, I established a **zero-shot baseline** to quantify the pre-trained model's performance. This provides a clear, numerical benchmark to measure the impact of my fine-tuning efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZTP8KxiGYIK",
    "outputId": "166e7ddb-bf3e-44d9-b2df-a4841ad6c421"
   },
   "outputs": [],
   "source": [
    "baseline_metrics = evaluate_vlm(model, processor, dataset_test_raw, max_samples=None, iou_threshold=0.5)\n",
    "print(baseline_metrics)\n",
    "\n",
    "# sanity checks below\n",
    "# run_inference(example)\n",
    "# from itertools import islice\n",
    "\n",
    "# for idx, example in enumerate(islice(dataset_test_raw, 10)):\n",
    "#   response = run_inference(example, max_new_tokens=256)\n",
    "#   pred_boxes = parse_bounding_boxes(response)\n",
    "#   gt_boxes = example[\"objects\"][\"bbox\"]\n",
    "\n",
    "#   print(f\"\\nSample {idx}\")\n",
    "#   print(\"Raw response:\")\n",
    "#   print(response)\n",
    "#   print(\"Decoded predicted boxes:\", pred_boxes)\n",
    "#   print(\"Ground-truth boxes:\", gt_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2hcPptiGYIK"
   },
   "outputs": [],
   "source": [
    "# def iou_debug(model, processor, dataset, num_samples=5):\n",
    "#   samples = islice(dataset, num_samples)\n",
    "#   for i, example in enumerate(samples):\n",
    "#       response = run_inference(example, max_new_tokens=256)\n",
    "#       preds = parse_bounding_boxes(response)\n",
    "#       gts = example[\"objects\"][\"bbox\"]\n",
    "\n",
    "#       if preds:\n",
    "#           gt = torch.tensor(gts, dtype=torch.float32)[:, [1,0,3,2]]\n",
    "#           pr = torch.tensor(preds, dtype=torch.float32)[:, [1,0,3,2]]\n",
    "#           ious = box_iou(gt, pr).max(dim=1).values.tolist()\n",
    "#       else:\n",
    "#           ious = [0.0] * len(gts)\n",
    "#       print(f\"Sample {i} IoUs:\", ious)\n",
    "\n",
    "# iou_debug(model, processor, dataset_test_raw, num_samples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47kpXETNGYIK"
   },
   "source": [
    "### Qualitative Analysis of Baseline Performance\n",
    "\n",
    "To provide a visual understanding of the baseline performance, I overlaid the model's predicted bounding box (in red) on top of the ground-truth box (in green) for a sample image.\n",
    "\n",
    "\n",
    "\n",
    "As shown, while the model correctly identifies the general region of the nutrition table, it lacks the precision needed for a practical application. The low IoU score for this sample visually corresponds to the significant misalignment between the two boxes. This qualitative result reinforces the need for fine-tuning to improve the model's localization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "voFNDRInGYIK",
    "outputId": "98157eb8-631f-4ff4-d055-26c840137878"
   },
   "outputs": [],
   "source": [
    "def visualize_prediction(example, response, title=\"Prediction vs. Ground Truth\"):\n",
    "  image = example[\"image\"].copy()\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  w, h = image.size\n",
    "\n",
    "  # Ground truth boxes come as [ymin, xmin, ymax, xmax]\n",
    "  for y_min, x_min, y_max, x_max in example[\"objects\"][\"bbox\"]:\n",
    "      draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"lime\",\n",
    "          width=4,\n",
    "      )\n",
    "\n",
    "  # Predictions from parse_bounding_boxes are [x_min, y_min, x_max, y_max]\n",
    "  for x_min, y_min, x_max, y_max in parse_bounding_boxes(response):\n",
    "      draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"red\",\n",
    "          width=4,\n",
    "      )\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.imshow(image)\n",
    "  plt.title(title)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# sample = dataset_test_raw[0]\n",
    "sample =dataset_train_raw[657]\n",
    "response = run_inference(sample, max_new_tokens=256)\n",
    "visualize_prediction(sample, response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG8NuzqjjbgI",
    "outputId": "71a903df-007e-4502-dcea-a2041809d456"
   },
   "source": [
    "<a id=\"strategy\"></a>\n",
    "## Fine-Tuning Strategy and Data Preparation\n",
    "\n",
    "With a clear baseline established, the next step is to fine-tune the model to improve its accuracy. This section outlines my strategy for training and the data preparation required.\n",
    "\n",
    "#### Training Objective vs. Evaluation Metric\n",
    "\n",
    "A key decision in this project is to **separate the training objective from the evaluation metric**.\n",
    "\n",
    "  * **Training Objective (Cross-Entropy Loss):** The model is trained to minimize **cross-entropy loss**, which measures the accuracy of token-by-token text prediction. It is a **differentiable** function, which is essential for backpropagation.\n",
    "      * **Limitation:** It is strict on **syntax**. The model is penalized for any textual deviation from the ground truth, even if the meaning (i.e., the bounding box coordinates) is identical.\n",
    "  * **Evaluation Metric (Mean IoU):** To measure true task success, I use **Mean IoU**, which calculates the geometric overlap between the predicted and ground-truth boxes. It is a direct measure of **geometric accuracy**.\n",
    "\n",
    "My approach is to train with cross-entropy loss but select the best checkpoint based on the **highest Mean IoU** on the validation set. This aligns the final model with the true task goal and helps monitor for overfitting.\n",
    "\n",
    "#### Fine-Tuning Experiments\n",
    "\n",
    "I will explore two LoRA strategies to determine the most effective fine-tuning approach:\n",
    "\n",
    "1.  **Language-Only LoRA**: Adapts only the LLM to better *interpret* the visual features.\n",
    "2.  **Vision+Language LoRA**: Adapts both the vision encoder and the LLM to *adapt and refine* the visual features themselves.\n",
    "\n",
    "#### Final Training Sample Structure\n",
    "\n",
    "The code below shows the final data structure that will be fed into the trainer. It combines the image, the engineered prompt, and the ground-truth assistant response with coordinates scaled to the required 1000x1000 format.\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      { \"type\": \"image\", \"image_url\": \"path/to/image.jpg\" },\n",
    "      { \"type\": \"text\", \"text\": \"Detect all nutrition label regions in this image. Respond with their bounding boxes using the format \\\"nutrition_label<box(x_min, y_min),(x_max, y_max)>\\\" on a 1000x1000 canvas. If there are multiple labels, return all of them on separate lines. Do not extract or describe any text \u2014 only detect and localize the label areas.\" }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"nutrition-table<box(250, 300),(450, 500)>\" # Example scaled coordinates\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNKJ1WH5GYIK",
    "outputId": "66f5eb3b-83fa-442c-8acf-a35e2edcdd9f"
   },
   "outputs": [],
   "source": [
    "# Reset GPU memory before (re)loading the base model + LoRA adapters\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxkXZuUkvy8j",
    "outputId": "3f179076-a578-41a1-aa35-372fd31e9ce1"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrR9gP2z90z"
   },
   "source": [
    "<a id=\"peft\"></a>\n",
    "## Rationale for Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Fine-tuning all 7 billion parameters of the Qwen2-VL model is not only impractical from a hardware perspective but also often suboptimal for performance. It risks **catastrophic forgetting**, where the model loses its powerful, general-purpose abilities, and can quickly overfit to a small dataset.\n",
    "\n",
    "Instead, I'm using **Parameter-Efficient Fine-Tuning (PEFT)** with **Low-Rank Adaptation (LoRA)**. This allows me to adapt the model by training less than 0.1% of its total parameters, preserving its core knowledge while teaching it our specific task.\n",
    "\n",
    "#### Why Full Fine-Tuning is Infeasible on an A100 40GB GPU\n",
    "\n",
    "A quick calculation demonstrates the memory constraints. For a 7-billion-parameter model, a full fine-tuning process requires VRAM for more than just the model weights:\n",
    "\n",
    "* **Model Weights (16-bit)**: 7B params \u00d7 2 bytes/param \u2248 **14 GB**\n",
    "* **Gradients (16-bit)**: 7B params \u00d7 2 bytes/param \u2248 **14 GB**\n",
    "* **Optimizer States (AdamW)**: 7B params \u00d7 8 bytes/param (for 32-bit moments) \u2248 **56 GB**\n",
    "\n",
    "The total, **~84 GB**, exceeds the **40 GB** or **80 GB** capacity of a A100 GPU before even accounting for the memory needed for activations, which is where your image data comes in. This makes full fine-tuning impossible.\n",
    "\n",
    "#### My Multi-Faceted Memory Optimization Strategy\n",
    "\n",
    "To solve this, I implemented a multi-faceted strategy where each component addresses a different part of the memory problem:\n",
    "\n",
    "1.  **LoRA & 8-bit Quantization**: This is the primary solution. By freezing the original weights and only training small LoRA adapters with an 8-bit optimizer (`paged_adamw_8bit`), I drastically reduce the memory needed for gradients and optimizer states from >70 GB to just a few hundred megabytes.\n",
    "2.  **`MAX_PIXELS` Image Resizing**: This addresses the **activation memory**. Even with LoRA, processing very high-resolution images can create large activation maps that cause out-of-memory (OOM) errors. By setting a maximum pixel count, I ensure that the memory required for the forward and backward passes remains within the GPU's limits, even for a `batch_size=1`.\n",
    "3.  **Gradient Checkpointing & Accumulation**: These techniques are the final polish. Gradient checkpointing trades compute time for memory, and accumulating gradients over 4 steps allows me to simulate a larger, more stable batch size of 4 without the associated memory cost.\n",
    "\n",
    "This deliberate, multi-pronged approach shows a clear understanding of the bottlenecks in VLM training and provides a robust solution.\n",
    "\n",
    "Of course. It's a great idea to document this decision. It shows you're being thoughtful about the trade-offs between data fidelity and hardware limitations.\n",
    "\n",
    "Here is the markdown you can add to your notebook, referencing the image distribution chart you've already created.\n",
    "\n",
    "\n",
    "### Pre-processing Strategy: Handling Variable Image Resolutions\n",
    "\n",
    "My analysis of the dataset revealed a wide distribution of image dimensions, with a long tail of very high-resolution images.\n",
    "\n",
    "\n",
    "These large outlier images can cause out-of-memory (OOM) errors during the initial data loading phase (`dataset.map()`), even before the trainer's optimizations are applied.\n",
    "\n",
    "To solve this, I've implemented a two-stage resizing strategy:\n",
    "\n",
    "1.  **Pre-emptive Resizing (Safety Net)**: Inside my `create_chat_format` function, I first cap the maximum size of any image by ensuring its longest side does not exceed **1024 pixels**. I chose `1024` as a balance between preserving as much visual detail as possible for the model to learn from, while still being a safe enough size to likely avoid OOM errors on the A100 40GB during data preparation.\n",
    "2.  **Final Resizing (`MAX_PIXELS`)**: After this initial safety check, the trainer's `vision_processor` takes over and applies the final `MAX_PIXELS = 470,000` constraint. This ensures every image fed into the training batch has a consistent memory footprint.\n",
    "\n",
    "This approach allows me to retain valuable detail from larger images while guaranteeing that the training process remains stable and within my VRAM budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f130f249da33433f953019f7315949bf",
      "005fde4a2cc346f29bf55acb1f8a27dd",
      "9e4c0e81f2bf4f83b3157eb6234eb8e5",
      "c907835d682049128abf463b8944dfba",
      "08cc1736d5674e8ba3fad23efc32b8ae",
      "b8d37078db834665a1d03ef6164b6d0f",
      "c9a73b093abe414dabb9be67e48db0d2",
      "d0fa140e02f5479eaed127ea542904f6",
      "9161c6826f034978a20cc4111222a5fb",
      "467c2561875947d089791f625f34e727",
      "4ee89175027748c6942af1a9aac29a36"
     ]
    },
    "id": "N4z1afjJGYIK",
    "outputId": "bc82b7da-a70d-4314-f891-9a384eaa300c"
   },
   "outputs": [],
   "source": [
    "DOWNSIZE = True\n",
    "\n",
    "def create_chat_format(sample):\n",
    "  \"\"\"\n",
    "  Converts a sample from the OpenFoodFacts dataset to the Qwen2-VL chat format.\n",
    "  *** This version correctly normalizes bounding box coordinates to a 0-1000 scale. ***\n",
    "  \"\"\"\n",
    "  assistant_response = \"\"\n",
    "  objects = sample[\"objects\"]\n",
    "\n",
    "  if DOWNSIZE:\n",
    "      max_long_side = 1024\n",
    "      img = sample[\"image\"].copy()\n",
    "      img.thumbnail((max_long_side, max_long_side), Image.Resampling.LANCZOS)\n",
    "      sample[\"image\"] = img\n",
    "\n",
    "  for i in range(len(objects[\"bbox\"])):\n",
    "      category = objects[\"category_name\"][i]\n",
    "      box = objects[\"bbox\"][i]\n",
    "\n",
    "      y_min_norm, x_min_norm, y_max_norm, x_max_norm = box\n",
    "\n",
    "      x_min = int(x_min_norm * 1000)\n",
    "      y_min = int(y_min_norm * 1000)\n",
    "      x_max = int(x_max_norm * 1000)\n",
    "      y_max = int(y_max_norm * 1000)\n",
    "\n",
    "      assistant_response += (\n",
    "          f\"<|object_ref_start|>{category}<|object_ref_end|>\"\n",
    "          f\"<|box_start|>({x_min},{y_min}),({x_max},{y_max})<|box_end|> \"\n",
    "      )\n",
    "\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "              {\"type\": \"text\", \"text\": USER_PROMPT},\n",
    "          ],\n",
    "      },\n",
    "      {\"role\": \"assistant\", \"content\": assistant_response.strip()},\n",
    "  ]\n",
    "\n",
    "  return {\"image\": sample[\"image\"], \"messages\": messages}\n",
    "\n",
    "\n",
    "print(\"Formatting training dataset...\")\n",
    "train_dataset = [create_chat_format(sample) for sample in dataset_train_raw]\n",
    "\n",
    "print(\"Formatting evaluation dataset...\")\n",
    "eval_dataset = [create_chat_format(sample) for sample in dataset_test_raw]\n",
    "\n",
    "print(f\"\u2705 Datasets formatted: {len(train_dataset)} train, {len(eval_dataset)} eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qo_Uu75GYIL",
    "outputId": "81b27d20-6565-4f83-ca26-aba8de5f75ee"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "print('\u2705 Flash Attention kernels enabled (flash_sdp).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSI_RMFaGYIS",
    "outputId": "ce18cdd0-771b-47e1-84b9-9e1bc0366e01"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# CRITICAL MEMORY FIX: Set MAX_PIXELS to constrain activation memory\n",
    "# ----------------------------------------------------------------------------------\n",
    "# The Qwen2-VL processor converts each image into a grid of patches. The total\n",
    "# number of patches is determined by the image's resolution. Without a cap,\n",
    "# high-resolution images can create an extremely large number of patches,\n",
    "# leading to out-of-memory errors from the activation maps during the forward pass.\n",
    "#\n",
    "# By setting MAX_PIXELS, we cap the total size of the feature map, which is the\n",
    "# primary lever for controlling VRAM usage from image data. This provides a\n",
    "# massive memory saving (~8-9 GB) compared to using original resolutions.\n",
    "#\n",
    "# A value of 470,400 (600 * 28 * 28) was chosen as a conservative but effective\n",
    "# setting for the A100 GPU.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "vision_process.MAX_PIXELS = 600 * 28 * 28\n",
    "print(f\"\u2705 MAX_PIXELS set to: {vision_process.MAX_PIXELS:,} pixels to manage VRAM.\")\n",
    "\n",
    "from qwen_vl_utils import process_vision_info, vision_process\n",
    "import torch\n",
    "\n",
    "# Verify MAX_PIXELS is set\n",
    "print(f\"MAX_PIXELS: {vision_process.MAX_PIXELS:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntWFqtNtZPyJ"
   },
   "source": [
    "<a id=\"training\"></a>\n",
    "## Fine-Tuning Experiments and Training\n",
    "\n",
    "Now I'll prepare the model for fine-tuning. This involves loading the model with 4-bit quantization to manage memory and then applying the LoRA configuration.\n",
    "\n",
    "#### Deconstructing the QLoRA Configuration\n",
    "\n",
    "The `BitsAndBytesConfig` is the core of QLoRA. Here's what the key choices mean:\n",
    "\n",
    "  * **`load_in_4bit=True`**: This instructs the library to load the large, frozen base model with its weights quantized to 4-bits, which is the primary source of memory savings.\n",
    "  * **`bnb_4bit_quant_type=\"nf4\"`**: I use the \"NormalFloat 4-bit\" (NF4) data type because it's specifically designed for the bell-curve distribution of neural network weights, offering better precision than standard 4-bit floats.\n",
    "  * **`bnb_4bit_compute_dtype=torch.bfloat16`**: This is a critical performance setting. It tells the model to de-quantize the 4-bit weights to 16-bit `bfloat16` for the actual matrix multiplications. GPUs have specialized hardware (Tensor Cores) optimized for 16-bit math, which provides a massive speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6d5cdbb40ca14b25b174d2034e1e8dfc",
      "6aacaf27d4af48e09245ba09ad9672d5",
      "a49d962b8a954676a547160a19a99926",
      "b1e81fd9cca949f6bc896c0b7a5ea30a",
      "494b2eb4c03746d780d9c27369d6cbb7",
      "618ca11131e1437990a9eaab1d44699f",
      "71cf1accec9f4bf7b01bbbacd4992029",
      "4405dcd698d342f3938188f48eb3e75d",
      "c04a40aa85004d03b77656071540790d",
      "6830117583f94e799e0372088384f65c",
      "650754f1f22241468e14266368b9f3d1"
     ]
    },
    "id": "7kgYkuY0GYIS",
    "outputId": "4701300c-fadd-460c-db1c-09b221b26ab6"
   },
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "print(\"\u2705 Vision-Language model and processor loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2y5Lr2xLG18"
   },
   "source": [
    "\n",
    "### Debugging an Out-of-Memory Error During Evaluation\n",
    "\n",
    "During my initial training run, I encountered an out-of-memory (OOM) error at the end of the first epoch, specifically when the validation step began.\n",
    "\n",
    "* **Problem Diagnosis**: The training itself was memory-stable, but during evaluation, the model would sometimes fail to generate an end-of-sequence token and produce an extremely long, unconstrained output. When the trainer tried to pad all validation predictions to match the length of this single long output, it attempted to allocate a massive tensor (~31 GB), causing the OOM crash.\n",
    "* **The Solution**: To fix this, I created a `GenerationConfig` object to explicitly control the generation behavior during the evaluation phase. By setting `max_new_tokens=128`, I provide a generous limit for the model to generate its short bounding box response, while preventing the runaway generation that caused the memory spike.\n",
    "\n",
    "This configuration is passed to the `SFTTrainer` to ensure all mid-training evaluations are memory-safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eai5Zb6aJUD1"
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,  # or 256 if you prefer\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model.generation_config = generation_config  # make it the default\n",
    "# print(hasattr(model, \"peft_config\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiQyx6CAKJZK"
   },
   "outputs": [],
   "source": [
    "class VLMDataCollator:\n",
    "  \"\"\"\n",
    "  Collate function for Qwen2-VL fine-tuning.\n",
    "\n",
    "  - Converts a mapped dataset example (with `messages` and `image`) into the\n",
    "    multimodal ChatML structure that Qwen expects: the user turn contains both the\n",
    "    image and the prompt text, and assistant turns carry plain text.\n",
    "  - Uses the Qwen processor to tokenize text and encode images, returning padded\n",
    "    batches with `input_ids`, `pixel_values`, and other multimodal features.\n",
    "  - Optionally masks the prompt tokens in `labels` (via `mask_prompt=True`) so that\n",
    "    the loss is computed only on the assistant\u2019s answer. This lets you switch between\n",
    "    completion loss and full-text loss without redefining the collator.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, processor, mask_prompt=True):\n",
    "      self.processor = processor\n",
    "      self.mask_prompt = mask_prompt\n",
    "      self.pad_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "  def _to_multimodal_chat(self, conversation, image):\n",
    "      formatted = []\n",
    "      for message in conversation:\n",
    "          role = message.get('role')\n",
    "          content = message.get('content')\n",
    "\n",
    "          if isinstance(content, list) and content and isinstance(content[0], dict) and 'type' in content[0]:\n",
    "              formatted.append(message)\n",
    "              continue\n",
    "\n",
    "          text = content if isinstance(content, str) else ''\n",
    "          if role == 'user':\n",
    "              formatted.append({\n",
    "                  'role': 'user',\n",
    "                  'content': [\n",
    "                      {'type': 'image', 'image': image},\n",
    "                      {'type': 'text', 'text': text.replace('<|image_1|>', '').strip()},\n",
    "                  ],\n",
    "              })\n",
    "          else:\n",
    "              formatted.append({\n",
    "                  'role': role,\n",
    "                  'content': [{'type': 'text', 'text': text}],\n",
    "              })\n",
    "      return formatted\n",
    "\n",
    "  def __call__(self, features):\n",
    "      processed_conversations = []\n",
    "      prompts = []\n",
    "      image_inputs = []\n",
    "\n",
    "      for feature in features:\n",
    "          conversation = feature['messages']\n",
    "          image = feature['image']\n",
    "\n",
    "          multimodal = self._to_multimodal_chat(conversation, image)\n",
    "          processed_conversations.append(multimodal)\n",
    "\n",
    "          prompts.append(\n",
    "              self.processor.apply_chat_template(\n",
    "                  multimodal, tokenize=False, add_generation_prompt=False\n",
    "              )\n",
    "          )\n",
    "\n",
    "          image_inputs.append(process_vision_info(multimodal)[0])\n",
    "\n",
    "      batch = self.processor(\n",
    "          text=prompts,\n",
    "          images=image_inputs,\n",
    "          return_tensors='pt',\n",
    "          padding=True,\n",
    "      )\n",
    "\n",
    "      batch['pixel_values'] = batch['pixel_values'].to(torch.bfloat16)\n",
    "\n",
    "      labels = batch['input_ids'].clone()\n",
    "      for idx, conversation in enumerate(processed_conversations):\n",
    "          prompt_only = conversation[:-1]\n",
    "          if not prompt_only:\n",
    "              continue\n",
    "          prompt_text = self.processor.apply_chat_template(\n",
    "              prompt_only, tokenize=False, add_generation_prompt=True\n",
    "          )\n",
    "          prompt_ids = self.processor.tokenizer(\n",
    "              prompt_text,\n",
    "              add_special_tokens=False,\n",
    "              return_attention_mask=False,\n",
    "          ).input_ids\n",
    "          if self.mask_prompt:\n",
    "              labels[idx, : len(prompt_ids)] = -100\n",
    "\n",
    "      if self.pad_id is not None:\n",
    "          labels[batch['input_ids'] == self.pad_id] = -100\n",
    "\n",
    "      batch['labels'] = labels\n",
    "      return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQhlmCYWKJZL"
   },
   "source": [
    "### Experiment Descriptions & Hypotheses\n",
    "\n",
    "  * **\u27a1\ufe0f Experiment 1a: Completion-Only Loss (Primary)**\n",
    "\n",
    "      * **Description**: LoRA on the LLM only, with loss calculated just on the assistant's answer.\n",
    "      * **Hypothesis**: This will be the most effective method, as the model's learning is focused purely on the task of generating correct bounding box strings.\n",
    "\n",
    "  * **\u27a1\ufe0f Experiment 1b: Full-Text Loss (Sanity Check)**\n",
    "\n",
    "      * **Description**: LoRA on the LLM only, but the loss is calculated over the entire conversation, including the prompt.\n",
    "      * **Hypothesis**: This will perform worse than **1a**, as the model will waste capacity learning to predict the prompt it was already given.\n",
    "\n",
    "  * **\u27a1\ufe0f Experiment 2: Vision + Language LoRA (Advanced)**\n",
    "\n",
    "      * **Description**: LoRA adapters are applied to both the vision encoder and the language model.\n",
    "      * **Hypothesis**: This may offer a slight improvement if the nutrition labels have distinct visual features not well-represented in the model's original pre-training data.\n",
    "\n",
    "#### Training Configuration (`SFTTrainer`)\n",
    "\n",
    "The `SFTConfig` is set up to balance performance and memory constraints on the A100 40GB GPU. Key choices include:\n",
    "\n",
    "  * **`gradient_accumulation_steps`**: This allows a larger effective batch size for more stable gradients without increasing VRAM.\n",
    "  * **`bf16=True`**: Enables automatic mixed-precision training, which speeds up computation significantly on modern GPUs.\n",
    "  * **`gradient_checkpointing=True`**: A memory-saving technique that trades some computation time to reduce VRAM needed for storing activations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf LoRA Target Modules: LLM vs Vision Encoder (Qwen2-VL)\n",
    "\n",
    "#### \u2705 Language Model (LLM) Layers\n",
    "- PEFT automatically matches **all layers** when you use simple strings like:\n",
    "  ```python\n",
    "  target_modules=[\"q_proj\", \"v_proj\"]\n",
    "  ```\n",
    "- Matches:  \n",
    "  `model.model.layers.0.self_attn.q_proj` \u2192 `...layers.27.self_attn.v_proj`\n",
    "\n",
    "> \ud83d\udca1 **Why these?**  \n",
    "> Research & practice show `q_proj` and `v_proj` are often the most impactful for LoRA in transformer attention blocks \u2014 tuning them gives ~90% of performance gain with minimal overhead.\n",
    "\n",
    "#### \ud83d\uddbc\ufe0f Vision Encoder Layers\n",
    "- Naming is different:  \n",
    "  `model.visual.blocks.0.attn.qkv` \u2192 `...blocks.31.attn.qkv`\n",
    "- Use **regex** to avoid accidental matches:\n",
    "  ```python\n",
    "  r\"visual\\.blocks\\.\\d+\\.attn\\.qkv\"\n",
    "  ```\n",
    "- \u26a0\ufe0f Avoid just `\"qkv\"` \u2014 too generic, may match unintended modules later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITmkRHWCKYjf"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# \ud83d\udd27 CRITICAL FIX #2: Reduce LoRA configuration for memory efficiency\n",
    "# ============================================================\n",
    "# Original config had:\n",
    "# - r=16 (rank 16)\n",
    "# - lora_alpha=32\n",
    "# - 7 target modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"]\n",
    "#\n",
    "# This consumed ~700 MB - 1 GB for LoRA adapters alone!\n",
    "#\n",
    "# New config (matching N's working notebook):\n",
    "# - r=8 (rank 8) \u2192 4x fewer parameters per adapter\n",
    "# - lora_alpha=16 (proportional to r)\n",
    "# - 2 target modules: [\"q_proj\", \"v_proj\"] \u2192 3.5x fewer modules\n",
    "#\n",
    "# Memory impact:\n",
    "# - Before: ~700 MB for LoRA + ~360 MB gradients = ~1.06 GB\n",
    "# - After:  ~200 MB for LoRA + ~100 MB gradients = ~0.30 GB\n",
    "# - Savings: ~760 MB!\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"v_proj\",\n",
    "        r\"visual\\.blocks\\.\\d+\\.attn\\.qkv\"  # \u2190 vision encoder attention, exp2\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJIjF4sQKJZL",
    "outputId": "1d3ff6c5-dd3e-42ff-c163-30a054e04897"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Training Configuration (`SFTConfig`)\n",
    "# ----------------------------------------------------------------------------------\n",
    "# The configuration below is optimized for a single A100 40GB GPU and implements\n",
    "# an early stopping strategy by saving the model at each epoch and loading the\n",
    "# best one at the end, based on the validation set's Mean IoU.\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Memory impact of gradient checkpointing:\n",
    "# - Without: ~9 GB for activations\n",
    "# - With:    ~0.6-1.0 GB for activations\n",
    "# - Savings: ~8 GB!\n",
    "#\n",
    "# Trade-off: ~20% slower training, but makes training POSSIBLE!\n",
    "\n",
    "\n",
    "# EXPERIMENT_NAME = 'exp1a'\n",
    "# EXPERIMENT_NAME = 'exp1b'\n",
    "EXPERIMENT_NAME = 'exp2'\n",
    "exp_tag = EXPERIMENT_NAME\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=f\"qwen2-7b-nutrition-a100_{exp_tag}\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,      # set to False for now\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# === Manual Evaluation Strategy ===\n",
    "# We disable automatic evaluation to prevent OOM errors and will\n",
    "# evaluate all saved checkpoints manually after training.\n",
    "sft_config.eval_strategy = \"no\" #\"epoch\"\n",
    "sft_config.load_best_model_at_end = False # having issues with in loop eval text generation control\n",
    "# sft_config.metric_for_best_model = \"eval_mean_gt_iou\"\n",
    "# sft_config.greater_is_better = True\n",
    "sft_config.generation_max_length = 128\n",
    "\n",
    "print(\"\u2705 SFTConfig created and optimized for single A100 with early stopping.\")\n",
    "print(f\"   Max epochs: {sft_config.num_train_epochs}\")\n",
    "print(f\"   Best model will be selected based on: {sft_config.metric_for_best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDmpXbkEGYIT",
    "outputId": "81df5c43-fa42-47ad-9895-38f11f58bf9c"
   },
   "outputs": [],
   "source": [
    "# MEMORY CHECK CELL\n",
    "\n",
    "if 'model' not in globals():\n",
    "    raise RuntimeError('Load the model before running this diagnostics cell.')\n",
    "\n",
    "try:\n",
    "    collator = vlm_collator\n",
    "except NameError:\n",
    "    collator = VLMDataCollator(processor)\n",
    "    vlm_collator = collator\n",
    "\n",
    "if 'batch_debug' not in locals():\n",
    "    sample = train_dataset[0]\n",
    "    batch_debug = collator([sample])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "bytes_per_param = 2  # assume bfloat16 params/checkpoints\n",
    "param_mem_gb = total_params * bytes_per_param / 1024**3\n",
    "trainable_mem_gb = trainable_params * bytes_per_param / 1024**3\n",
    "\n",
    "seq_len = batch_debug['input_ids'].shape[-1]\n",
    "hidden_size = model.config.text_config.hidden_size\n",
    "bytes_per_activation = 2  # bfloat16 activations\n",
    "activation_mem_gb = (seq_len * hidden_size * bytes_per_activation *\n",
    "                     sft_config.per_device_train_batch_size) / 1024**3\n",
    "\n",
    "free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "free_mem_gb, total_mem_gb = free_mem / 1024**3, total_mem / 1024**3\n",
    "\n",
    "print(f'Total params: {total_params:,} (~{param_mem_gb:.2f} GB)')\n",
    "print(f'Trainable params: {trainable_params:,} (~{trainable_mem_gb:.2f} GB)')\n",
    "print(f'Sequence length (debug batch): {seq_len}')\n",
    "print(f'Hidden size: {hidden_size}')\n",
    "print(f'Per-microbatch activation estimate: ~{activation_mem_gb:.2f} GB')\n",
    "print(f'Gradient accumulation steps: {sft_config.gradient_accumulation_steps}')\n",
    "print(f'Effective batch size: {sft_config.gradient_accumulation_steps * sft_config.per_device_train_batch_size}')\n",
    "print(f'CUDA memory (free/total): {free_mem_gb:.2f} / {total_mem_gb:.2f} GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFmoqlK7GYIT",
    "outputId": "7bb05041-da9e-4580-ade3-88643b12fd77"
   },
   "outputs": [],
   "source": [
    "mask_prompt = EXPERIMENT_NAME != 'exp1b' #should be true for exp1a and 2\n",
    "vlm_collator = VLMDataCollator(processor, mask_prompt=mask_prompt)\n",
    "print(f'\u2705 Collator ready for {EXPERIMENT_NAME} (mask_prompt={mask_prompt})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8v-fXp91vRQ"
   },
   "outputs": [],
   "source": [
    "# can be used in training loop eval\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = processor.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 with pad token id in a copy of labels, then decode\n",
    "    labels_copy = labels.copy()\n",
    "    labels_copy[labels_copy == -100] = processor.tokenizer.pad_token_id\n",
    "    decoded_labels = processor.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "\n",
    "    total_iou = 0.0\n",
    "    tp = fp = fn = 0\n",
    "    total_gt = 0\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    for pred_text, label_text in zip(decoded_preds, decoded_labels):\n",
    "        pred_boxes = parse_bounding_boxes(pred_text)  # [x_min, y_min, x_max, y_max]\n",
    "        gt_boxes = parse_bounding_boxes(label_text)   # same format now\n",
    "        if not gt_boxes and not pred_boxes:\n",
    "            continue\n",
    "        if not pred_boxes:\n",
    "            fn += len(gt_boxes)\n",
    "            total_gt += len(gt_boxes)\n",
    "            continue\n",
    "        if not gt_boxes:\n",
    "            fp += len(pred_boxes)\n",
    "            continue\n",
    "\n",
    "        pred_tensor = torch.tensor(pred_boxes, dtype=torch.float32)\n",
    "        gt_tensor = torch.tensor(gt_boxes, dtype=torch.float32)\n",
    "\n",
    "        iou_matrix = box_iou(pred_tensor, gt_tensor)\n",
    "        if iou_matrix.numel() == 0:\n",
    "            fn += len(gt_boxes)\n",
    "            fp += len(pred_boxes)\n",
    "            total_gt += len(gt_boxes)\n",
    "            continue\n",
    "\n",
    "        # greedy match\n",
    "        all_pairs = [\n",
    "            (iou_matrix[p, g].item(), p, g)\n",
    "            for p in range(iou_matrix.shape[0])\n",
    "            for g in range(iou_matrix.shape[1])\n",
    "        ]\n",
    "        all_pairs.sort(reverse=True)\n",
    "\n",
    "        matched_preds = set()\n",
    "        matched_gts = set()\n",
    "        matched_iou_sum = 0.0\n",
    "        for iou, p, g in all_pairs:\n",
    "            if iou < iou_threshold:\n",
    "                break\n",
    "            if p in matched_preds or g in matched_gts:\n",
    "                continue\n",
    "            matched_preds.add(p)\n",
    "            matched_gts.add(g)\n",
    "            matched_iou_sum += iou\n",
    "\n",
    "        tp += len(matched_preds)\n",
    "        fp += len(pred_boxes) - len(matched_preds)\n",
    "        fn += len(gt_boxes) - len(matched_preds)\n",
    "\n",
    "        total_iou += matched_iou_sum\n",
    "        total_gt += len(gt_boxes)\n",
    "\n",
    "    mean_iou = total_iou / total_gt if total_gt else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"mean_gt_iou\": mean_iou,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Zd_hYCy6KJZL",
    "outputId": "aeebc85f-997c-4ed2-f0af-36128547bc01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=vlm_collator,\n",
    "    peft_config=peft_config,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.model.print_trainable_parameters()  # just to confirm LoRA is live\n",
    "train_output = trainer.train()\n",
    "# print(train_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1-l7PzQh2AP"
   },
   "source": [
    "### Evaluation Setup: SDPA Attention Implementation\n",
    "\n",
    "For all evaluations (baseline and fine-tuned checkpoints), I used **SDPA (Scaled Dot Product Attention)** which is PyTorch's native attention implementation:\n",
    "\n",
    "```python\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "  \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    "  attn_implementation=\"sdpa\",  # Use PyTorch SDPA\n",
    ")\n",
    "\n",
    "Why SDPA instead of Flash Attention?\n",
    "- Compatibility: Works reliably with 4-bit quantization + bfloat16\n",
    "- Stability: No kernel fallback issues during inference\n",
    "- Consistency: Same attention mechanism across all evaluations (baseline + experiments)\n",
    "- Sufficient Performance: Evaluation is not bottlenecked by attention (model loading takes longer)\n",
    "\n",
    "Training vs Evaluation:\n",
    "- Training: Used default attention (Flash Attention when available) for maximum memory efficiency\n",
    "- Evaluation: Explicitly specified SDPA for consistent, stable inference\n",
    "\n",
    "This ensures apples-to-apples comparison across all checkpoints and the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJaxx7S5U2Xd"
   },
   "outputs": [],
   "source": [
    "def downsize_images(sample):\n",
    "  \"\"\"Only resize images, keep everything else intact\"\"\"\n",
    "  max_long_side = 1024\n",
    "  img = sample[\"image\"].copy()\n",
    "  img.thumbnail((max_long_side, max_long_side), Image.Resampling.LANCZOS)\n",
    "  sample[\"image\"] = img\n",
    "  return sample\n",
    "\n",
    "# Apply downsizing to RAW dataset (this keeps \"objects\" field)\n",
    "dataset_test_downsized = [downsize_images(sample) for sample in dataset_test_raw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## Checkpoint Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECKPOINT EVALUATION - Find Best Model Using evaluate_vlm\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "This cell evaluates all training checkpoints to find the best performing model.\n",
    "\n",
    "WHY evaluate_vlm():\n",
    "- Ensures ALL ground truth boxes are counted (matched or not)\n",
    "- Unmatched GT boxes contribute 0 to IoU (included in denominator)\n",
    "\n",
    "Example: If image has 3 GT boxes but model predicts 1:\n",
    "- 1 matched box contributes its IoU (e.g., 0.8)\n",
    "- 2 unmatched boxes contribute 0.0\n",
    "- mean_iou = 0.8 / 3 = 0.267\n",
    "\"\"\"\n",
    "\n",
    "EXPERIMENT_NAME = 'exp2'  # CHANGE THIS: 'exp1a', 'exp1b', or 'exp2'\n",
    "output_dir = f\"qwen2-7b-nutrition-a100_{EXPERIMENT_NAME}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\ud83d\udd0d Evaluating {EXPERIMENT_NAME} checkpoints with evaluate_vlm\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Load processor (shared across all checkpoints)\n",
    "# ============================================================================\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# Set MAX_PIXELS\n",
    "vision_process.MAX_PIXELS = 600 * 28 * 28\n",
    "print(f\"\u2705 MAX_PIXELS set to: {vision_process.MAX_PIXELS:,} pixels\")\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Find all checkpoint directories\n",
    "# ============================================================================\n",
    "all_items = os.listdir(output_dir)\n",
    "\n",
    "def extract_checkpoint_number(checkpoint_name):\n",
    "  \"\"\"\n",
    "  Extract step number from checkpoint name.\n",
    "  \n",
    "  Args:\n",
    "      checkpoint_name: String like 'checkpoint-271'\n",
    "  \n",
    "  Returns:\n",
    "      int: Step number (271) or None if not a valid checkpoint\n",
    "  \"\"\"\n",
    "  try:\n",
    "      return int(checkpoint_name.split('-')[1])\n",
    "  except (IndexError, ValueError):\n",
    "      return None\n",
    "\n",
    "# Filter only valid checkpoints and sort numerically\n",
    "valid_checkpoints = [d for d in all_items if extract_checkpoint_number(d) is not None]\n",
    "checkpoints = sorted(valid_checkpoints, key=extract_checkpoint_number)\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Found {len(checkpoints)} checkpoints to evaluate\")\n",
    "print(f\"   Range: {checkpoints[0]} to {checkpoints[-1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Evaluate each checkpoint\n",
    "# ============================================================================\n",
    "checkpoint_results = []\n",
    "\n",
    "for i, checkpoint in enumerate(checkpoints, 1):\n",
    "  checkpoint_path = os.path.join(output_dir, checkpoint)\n",
    "  step = extract_checkpoint_number(checkpoint)\n",
    "\n",
    "  print(f\"\\n[{i}/{len(checkpoints)}] Evaluating {checkpoint} (step {step})...\")\n",
    "\n",
    "    \n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  )\n",
    "\n",
    "  # Load base model (quantization apples to apples comparison)\n",
    "  base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "      \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      quantization_config=bnb_config,\n",
    "      device_map=\"auto\",\n",
    "      attn_implementation=\"sdpa\",\n",
    "  )\n",
    "\n",
    "  # Load LoRA adapter weights\n",
    "  model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "  # Evaluate with evaluate_vlm (consistent with baseline)\n",
    "  metrics = evaluate_vlm(\n",
    "      model,\n",
    "      processor,\n",
    "      dataset_test_downsized,  # Same test set as experiments\n",
    "      max_samples=None,         # Evaluate all 123 samples\n",
    "      iou_threshold=0.5         # Standard threshold for detection\n",
    "  )\n",
    "\n",
    "  # Store results\n",
    "  checkpoint_results.append({\n",
    "      'checkpoint': checkpoint,\n",
    "      'checkpoint_step': step,\n",
    "      'mean_gt_iou': metrics['mean_gt_iou'],      # Mean IoU over ALL GT boxes\n",
    "      'precision@0.5': metrics['precision@0.50'],  # TP / (TP + FP)\n",
    "      'recall@0.5': metrics['recall@0.50'],        # TP / (TP + FN)\n",
    "      'f1@0.5': metrics['f1@0.50'],                # Harmonic mean\n",
    "  })\n",
    "\n",
    "  print(f\"   Mean GT IoU: {metrics['mean_gt_iou']:.3f}\")\n",
    "  print(f\"   Precision:   {metrics['precision@0.50']:.3f}\")\n",
    "  print(f\"   Recall:      {metrics['recall@0.50']:.3f}\")\n",
    "  print(f\"   F1 Score:    {metrics['f1@0.50']:.3f}\")\n",
    "\n",
    "  # Clean up GPU memory\n",
    "  del model\n",
    "  del base_model\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Find best checkpoint and save results\n",
    "# ============================================================================\n",
    "df = pd.DataFrame(checkpoint_results)\n",
    "df = df.sort_values('checkpoint_step')\n",
    "\n",
    "# Save detailed results\n",
    "results_path = os.path.join(output_dir, f'{EXPERIMENT_NAME}_checkpoint_results.csv')\n",
    "df.to_csv(results_path, index=False)\n",
    "print(f\"\\n\ud83d\udcbe Saved results to: {results_path}\")\n",
    "\n",
    "# Find best checkpoint by mean GT IoU\n",
    "best_idx = df['mean_gt_iou'].idxmax()\n",
    "best_checkpoint = df.loc[best_idx, 'checkpoint']\n",
    "best_iou = df.loc[best_idx, 'mean_gt_iou']\n",
    "best_f1 = df.loc[best_idx, 'f1@0.5']\n",
    "best_step = df.loc[best_idx, 'checkpoint_step']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\ud83c\udfc6 BEST CHECKPOINT: {best_checkpoint}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Step:        {best_step}\")\n",
    "print(f\"   Mean GT IoU: {best_iou:.3f}\")\n",
    "print(f\"   F1 Score:    {best_f1:.3f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display all results in compact format\n",
    "print(f\"\\n\ud83d\udcca All Checkpoint Results:\")\n",
    "print(df[['checkpoint_step', 'mean_gt_iou', 'f1@0.5']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n\u2705 Checkpoint evaluation complete for {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_boxes, gt_boxes):\n",
    "  \"\"\"\n",
    "  Calculate mean IoU between predicted and ground truth boxes\n",
    "  \n",
    "  Args:\n",
    "      pred_boxes: List of [x_min, y_min, x_max, y_max] (normalized, corner format)\n",
    "      gt_boxes: List of [y_min, x_min, y_max, x_max] (normalized, corner format)\n",
    "  \"\"\"\n",
    "  if not pred_boxes or not gt_boxes:\n",
    "      return 0.0\n",
    "\n",
    "  ious = []\n",
    "  for gt_box in gt_boxes:\n",
    "      # GT format: [y_min, x_min, y_max, x_max] -> convert to [x_min, y_min, x_max, y_max]\n",
    "      gt_y_min, gt_x_min, gt_y_max, gt_x_max = gt_box\n",
    "\n",
    "      best_iou = 0.0\n",
    "      for pred_box in pred_boxes:\n",
    "          # Pred format: [x_min, y_min, x_max, y_max]\n",
    "          pred_x_min, pred_y_min, pred_x_max, pred_y_max = pred_box\n",
    "\n",
    "          # Calculate intersection (both now in same coordinate system)\n",
    "          x_left = max(gt_x_min, pred_x_min)\n",
    "          y_top = max(gt_y_min, pred_y_min)\n",
    "          x_right = min(gt_x_max, pred_x_max)\n",
    "          y_bottom = min(gt_y_max, pred_y_max)\n",
    "\n",
    "          if x_right > x_left and y_bottom > y_top:\n",
    "              intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "              # Calculate areas\n",
    "              gt_area = (gt_x_max - gt_x_min) * (gt_y_max - gt_y_min)\n",
    "              pred_area = (pred_x_max - pred_x_min) * (pred_y_max - pred_y_min)\n",
    "\n",
    "              union = gt_area + pred_area - intersection\n",
    "              iou = intersection / union if union > 0 else 0.0\n",
    "              best_iou = max(best_iou, iou)\n",
    "\n",
    "      ious.append(best_iou)\n",
    "\n",
    "  return sum(ious) / len(ious) if ious else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_ious(model, processor, dataset, max_samples=None):\n",
    "  \"\"\"\n",
    "  Calculate IoU for each sample individually.\n",
    "  \n",
    "  This function runs inference on each test sample and calculates the IoU\n",
    "  between predicted and ground truth boxes. Used for:\n",
    "  - Distribution analysis\n",
    "  - Failure case identification\n",
    "  - Individual sample visualization\n",
    "  \n",
    "  Args:\n",
    "      model: Fine-tuned or baseline model\n",
    "      processor: AutoProcessor for the model\n",
    "      dataset: Test dataset (downsized)\n",
    "      max_samples: Optional limit on samples to process\n",
    "  \n",
    "  Returns:\n",
    "      DataFrame with columns: sample_idx, image_id, iou, prediction, pred_boxes, gt_boxes\n",
    "  \"\"\"\n",
    "  sample_results = []\n",
    "  samples = dataset[:max_samples] if max_samples else dataset\n",
    "\n",
    "  for idx, example in enumerate(samples):\n",
    "      response = run_inference(example, model=model, processor=processor)\n",
    "      pred_boxes = parse_bounding_boxes(response)\n",
    "      gt_boxes = example[\"objects\"][\"bbox\"]\n",
    "      iou = calculate_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "      sample_results.append({\n",
    "          'sample_idx': idx,\n",
    "          'image_id': example.get('image_id', f'sample_{idx}'),\n",
    "          'iou': iou,\n",
    "          'prediction': response,\n",
    "          'pred_boxes': pred_boxes,\n",
    "          'gt_boxes': gt_boxes\n",
    "      })\n",
    "\n",
    "      if (idx + 1) % 20 == 0:\n",
    "          print(f\"   Processed {idx + 1}/{len(samples)} samples...\")\n",
    "\n",
    "  return pd.DataFrame(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# COMPLETE EXPERIMENT ANALYSIS - ALL IN ONE CELL\n",
    "# \u26a0\ufe0f IMPORTANT: Change EXPERIMENT_NAME for each run!\n",
    "# ============================================================\n",
    "\n",
    "# \u2705 SET THIS - Change for each experiment: 'exp1a', 'exp1b', 'exp2'\n",
    "# EXPERIMENT_NAME = 'exp1b'\n",
    "EXPERIMENT_NAME = 'exp2'\n",
    "\n",
    "output_dir = f\"qwen2-7b-nutrition-a100_{EXPERIMENT_NAME}\"\n",
    "base_model_id = 'Qwen/Qwen2-VL-7B-Instruct'\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ANALYZING EXPERIMENT: {EXPERIMENT_NAME}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Set MAX_PIXELS\n",
    "vision_process.MAX_PIXELS = 600 * 28 * 28\n",
    "print(f\"\u2705 MAX_PIXELS set to: {vision_process.MAX_PIXELS:,} pixels\")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "# Disable SDPA\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Load checkpoint evaluation results\n",
    "# ============================================================\n",
    "\n",
    "results_path = os.path.join(output_dir, f'{EXPERIMENT_NAME}_checkpoint_results.csv')\n",
    "\n",
    "# Check if results exist\n",
    "if not os.path.exists(results_path):\n",
    "  print(f\"\u274c Results file not found: {results_path}\")\n",
    "  print(f\"   Run checkpoint evaluation first!\")\n",
    "  raise FileNotFoundError(results_path)\n",
    "\n",
    "df = pd.read_csv(results_path)\n",
    "df['checkpoint_step'] = df['checkpoint'].str.extract(r'(\\d+)').astype(int)\n",
    "df_sorted = df.sort_values('checkpoint_step')\n",
    "\n",
    "\n",
    "best_checkpoint = df.loc[df[\"mean_gt_iou\"].idxmax(), \"checkpoint\"]\n",
    "best_iou = df.loc[df['mean_gt_iou'].idxmax(), 'mean_gt_iou']\n",
    "\n",
    "print(f\"\u2705 Loaded results from: {results_path}\")\n",
    "print(f\"\u2705 Best checkpoint: {best_checkpoint} (IoU: {best_iou:.4f})\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Load training history\n",
    "# ============================================================\n",
    "\n",
    "trainer_state_path = os.path.join(output_dir, best_checkpoint, \"trainer_state.json\")\n",
    "\n",
    "if not os.path.exists(trainer_state_path):\n",
    "  print(f\"\u274c Training state not found: {trainer_state_path}\")\n",
    "  raise FileNotFoundError(trainer_state_path)\n",
    "\n",
    "with open(trainer_state_path) as f:\n",
    "  trainer_state = json.load(f)\n",
    "\n",
    "history = pd.DataFrame(trainer_state[\"log_history\"])\n",
    "train_loss = history.loc[history[\"loss\"].notna(), [\"step\", \"loss\"]].copy()\n",
    "train_loss[\"epoch\"] = train_loss[\"step\"] / 271\n",
    "\n",
    "print(f\"\u2705 Loaded training history\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Plot training progress\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Generating training progress plot...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle(f'{EXPERIMENT_NAME}: Training Progress', fontsize=16)\n",
    "\n",
    "# Training loss\n",
    "_= ax1.plot(train_loss[\"epoch\"], train_loss[\"loss\"], linewidth=2, color='#2E86AB')\n",
    "_= ax1.set_xlabel('Epoch', fontsize=12)\n",
    "_= ax1.set_ylabel('Training Loss (Cross Entropy)', fontsize=12)\n",
    "_= ax1.set_title('Training Loss', fontsize=14)\n",
    "_= ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation metrics\n",
    "_= ax2_twin = ax2.twinx()\n",
    "_= line1 = ax2.plot(df_sorted[\"checkpoint_step\"] / 271, df_sorted[\"mean_gt_iou\"],\n",
    "               marker='o', linewidth=2, markersize=6, color='#A23B72', label='Mean IoU')\n",
    "_= ax2.set_xlabel('Epoch', fontsize=12)\n",
    "_= ax2.set_ylabel('Mean IoU', fontsize=12, color='#A23B72')\n",
    "_= ax2.tick_params(axis='y', labelcolor='#A23B72')\n",
    "\n",
    "# print(df_sorted)\n",
    "\n",
    "line2 = ax2_twin.plot(df_sorted[\"checkpoint_step\"] / 271, df_sorted[\"f1@0.5\"],\n",
    "                    marker='s', linewidth=2, markersize=6, color='#F18F01', label='F1 Score')\n",
    "_= ax2_twin.set_ylabel('F1 Score', fontsize=12, color='#F18F01')\n",
    "_= ax2_twin.tick_params(axis='y', labelcolor='#F18F01')\n",
    "\n",
    "_= ax2.set_title('Validation Metrics', fontsize=14)\n",
    "_= ax2.grid(True, alpha=0.3)\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax2.legend(lines, labels, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(output_dir, f'training_validation_{EXPERIMENT_NAME}.png')\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\u2705 Saved: {plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Load best model for sample analysis\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Loading best model: {best_checkpoint}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "  base_model_id,\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    "  attn_implementation=\"sdpa\",  # \u2190 This uses SDPA anyway\n",
    "  trust_remote_code=True,\n",
    ")\n",
    "\n",
    "best_ckpt_path = os.path.join(output_dir, best_checkpoint)\n",
    "best_model = PeftModel.from_pretrained(base_model, best_ckpt_path, is_trainable=False)\n",
    "_ = best_model.eval()\n",
    "\n",
    "print(f\"\u2705 Model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Sample IoU analysis\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Analyzing sample-level IoUs...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "sample_df = get_sample_ious(best_model, processor, dataset_test_downsized)\n",
    "\n",
    "# Save sample IoUs\n",
    "sample_iou_path = os.path.join(output_dir, f'sample_ious_{EXPERIMENT_NAME}.csv')\n",
    "sample_df.to_csv(sample_iou_path, index=False)\n",
    "print(f\"\u2705 Saved sample IoUs: {sample_iou_path}\")\n",
    "\n",
    "# Stratified sampling\n",
    "sample_df_sorted = sample_df.sort_values('iou')\n",
    "\n",
    "bottom_quartile = sample_df_sorted.iloc[:len(sample_df)//4]\n",
    "worst_samples = bottom_quartile.nsmallest(3, 'iou')['sample_idx'].tolist()\n",
    "\n",
    "middle_quartiles = sample_df_sorted.iloc[len(sample_df)//4:3*len(sample_df)//4]\n",
    "median_samples = middle_quartiles.sample(3, random_state=42)['sample_idx'].tolist()\n",
    "\n",
    "top_quartile = sample_df_sorted.iloc[3*len(sample_df)//4:]\n",
    "best_samples = top_quartile.nlargest(3, 'iou')['sample_idx'].tolist()\n",
    "\n",
    "print(f\"\\nWorst samples (0-25%): {worst_samples}\")\n",
    "print(f\"  IoU: {[sample_df.loc[sample_df['sample_idx'] == i, 'iou'].values[0] for i in worst_samples]}\")\n",
    "print(f\"\\nMedian samples (25-75%): {median_samples}\")\n",
    "print(f\"  IoU: {[sample_df.loc[sample_df['sample_idx'] == i, 'iou'].values[0] for i in median_samples]}\")\n",
    "print(f\"\\nBest samples (75-100%): {best_samples}\")\n",
    "print(f\"  IoU: {[sample_df.loc[sample_df['sample_idx'] == i, 'iou'].values[0] for i in best_samples]}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: Visualize performance distribution\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Generating performance distribution visualization...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "fig.suptitle(f'{EXPERIMENT_NAME}: Performance Distribution (Worst \u2192 Median \u2192 Best)', fontsize=16)\n",
    "\n",
    "all_samples = worst_samples + median_samples + best_samples\n",
    "sample_labels = ['Worst'] * 3 + ['Median'] * 3 + ['Best'] * 3\n",
    "\n",
    "for idx, (sample_idx, label, ax) in enumerate(zip(all_samples, sample_labels, axes.flat)):\n",
    "  sample = dataset_test_downsized[sample_idx]\n",
    "  response = run_inference(sample, model=best_model, processor=processor, max_new_tokens=128)\n",
    "\n",
    "  image = sample[\"image\"].copy()\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  w, h = image.size\n",
    "\n",
    "  # GT (green)\n",
    "  for y_min, x_min, y_max, x_max in sample[\"objects\"][\"bbox\"]:\n",
    "    _= draw.rectangle([(x_min * w, y_min * h), (x_max * w, y_max * h)], outline=\"lime\", width=3);\n",
    "\n",
    "  # Pred (red)\n",
    "  pred_boxes = parse_bounding_boxes(response)\n",
    "  for x_min, y_min, x_max, y_max in pred_boxes:\n",
    "      draw.rectangle([(x_min * w, y_min * h), (x_max * w, y_max * h)], outline=\"red\", width=3);\n",
    "\n",
    "  iou = sample_df.loc[sample_df['sample_idx'] == sample_idx, 'iou'].values[0]\n",
    "  _= ax.imshow(image)\n",
    "  _= ax.axis('off')\n",
    "  _= ax.set_title(f'{label} - IoU: {iou:.3f}', fontsize=11);\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_path = os.path.join(output_dir, f'failure_analysis_{EXPERIMENT_NAME}.png')\n",
    "plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\u2705 Saved: {viz_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 8: IoU Distribution Analysis (Bimodal Check)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca PART 8: IoU Distribution Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot IoU distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5));\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(sample_df['iou'], bins=30, edgecolor='black', alpha=0.7);\n",
    "axes[0].axvline(sample_df['iou'].mean(), color='red', linestyle='--',\n",
    "              linewidth=2, label=f'Mean: {sample_df[\"iou\"].mean():.3f}');\n",
    "axes[0].axvline(sample_df['iou'].median(), color='green', linestyle='--',\n",
    "              linewidth=2, label=f'Median: {sample_df[\"iou\"].median():.3f}');\n",
    "axes[0].set_xlabel('IoU Score', fontsize=12);\n",
    "axes[0].set_ylabel('Frequency', fontsize=12);\n",
    "axes[0].set_title('IoU Distribution - Test Set', fontsize=14, fontweight='bold');\n",
    "axes[0].legend(fontsize=11);\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_ious = np.sort(sample_df['iou'])\n",
    "cumulative = np.arange(1, len(sorted_ious) + 1) / len(sorted_ious) * 100\n",
    "axes[1].plot(sorted_ious, cumulative, linewidth=2);\n",
    "axes[1].axhline(80, color='red', linestyle='--', alpha=0.5, label='80th percentile');\n",
    "axes[1].axhline(50, color='green', linestyle='--', alpha=0.5, label='50th percentile');\n",
    "axes[1].set_xlabel('IoU Score', fontsize=12);\n",
    "axes[1].set_ylabel('Cumulative Percentage', fontsize=12);\n",
    "axes[1].set_title('Cumulative IoU Distribution', fontsize=14, fontweight='bold');\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'{EXPERIMENT_NAME}_iou_distribution.png'),\n",
    "          dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n\ud83d\udcc8 Distribution Statistics:\")\n",
    "print(f\"   Mean IoU:   {sample_df['iou'].mean():.3f}\")\n",
    "print(f\"   Median IoU: {sample_df['iou'].median():.3f}\")\n",
    "print(f\"   Std Dev:    {sample_df['iou'].std():.3f}\")\n",
    "print(f\"\\n   Min IoU:    {sample_df['iou'].min():.3f}\")\n",
    "print(f\"   Max IoU:    {sample_df['iou'].max():.3f}\")\n",
    "\n",
    "# Quartile breakdown\n",
    "q1 = sample_df['iou'].quantile(0.25)\n",
    "q2 = sample_df['iou'].quantile(0.50)\n",
    "q3 = sample_df['iou'].quantile(0.75)\n",
    "\n",
    "print(f\"\\n   25th percentile: {q1:.3f}\")\n",
    "print(f\"   50th percentile: {q2:.3f}\")\n",
    "print(f\"   75th percentile: {q3:.3f}\")\n",
    "\n",
    "# Performance buckets\n",
    "excellent = (sample_df['iou'] >= 0.8).sum()\n",
    "good = ((sample_df['iou'] >= 0.6) & (sample_df['iou'] < 0.8)).sum()\n",
    "poor = ((sample_df['iou'] >= 0.3) & (sample_df['iou'] < 0.6)).sum()\n",
    "failures = (sample_df['iou'] < 0.3).sum()\n",
    "\n",
    "total = len(sample_df)\n",
    "print(f\"\\n   Performance Buckets:\")\n",
    "print(f\"   Excellent (\u22650.8): {excellent:3d} ({excellent/total*100:.1f}%)\")\n",
    "print(f\"   Good (0.6-0.8):   {good:3d} ({good/total*100:.1f}%)\")\n",
    "print(f\"   Poor (0.3-0.6):   {poor:3d} ({poor/total*100:.1f}%)\")\n",
    "print(f\"   Failures (<0.3):  {failures:3d} ({failures/total*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 9: Save All Predictions as Images\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcbe PART 9: Saving All Predictions as PNGs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_viz_dir = os.path.join(output_dir, 'all_predictions')\n",
    "os.makedirs(output_viz_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving {len(sample_df)} prediction visualizations...\")\n",
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "  sample = dataset_test_downsized[row['sample_idx']]\n",
    "\n",
    "  # Use the working visualization approach\n",
    "  image = sample[\"image\"].copy()\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  w, h = image.size\n",
    "\n",
    "  # Ground truth boxes (normalized [ymin, xmin, ymax, xmax])\n",
    "  for y_min, x_min, y_max, x_max in sample[\"objects\"][\"bbox\"]:\n",
    "      draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"lime\",\n",
    "          width=4,\n",
    "      );\n",
    "\n",
    "  # Predicted boxes (from saved prediction text)\n",
    "  pred_boxes = parse_bounding_boxes(row['prediction'])\n",
    "  for x_min, y_min, x_max, y_max in pred_boxes:\n",
    "    _= draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"red\",\n",
    "          width=4,\n",
    "      );\n",
    "\n",
    "  # Create matplotlib figure to save with title\n",
    "  _= fig, ax = plt.subplots(figsize=(10, 8));\n",
    "  _= ax.imshow(image);\n",
    "  _= ax.set_title(f\"IoU: {row['iou']:.3f} | Image ID: {row['image_id']}\",\n",
    "               fontsize=14, fontweight='bold');\n",
    "  _= ax.axis('off');\n",
    "\n",
    "  # Add legend\n",
    "  handles = [\n",
    "      plt.Line2D([0], [0], color='lime', linewidth=3, label='Ground Truth'),\n",
    "      plt.Line2D([0], [0], color='red', linewidth=3, label='Prediction')\n",
    "  ]\n",
    "  _= ax.legend(handles=handles, loc='upper right', fontsize=10);\n",
    "\n",
    "  # Save\n",
    "  filename = f\"{row['iou']:.3f}_{row['image_id']}.png\"\n",
    "  _=plt.savefig(os.path.join(output_viz_dir, filename),\n",
    "              bbox_inches='tight', dpi=100);\n",
    "  plt.close()\n",
    "\n",
    "  # Don't use plt.show() - it causes hanging!\n",
    "\n",
    "print(f\"\u2705 Saved {len(sample_df)} images to: {output_viz_dir}\")\n",
    "print(f\"   Files sorted by IoU (worst to best)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\u2705 {EXPERIMENT_NAME.upper()} ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83d\udcc1 All outputs saved to: {output_dir}/\")\n",
    "print(f\"   \u2022 Training plot: {EXPERIMENT_NAME}_training_plot.png\")\n",
    "print(f\"   \u2022 Failure cases: {EXPERIMENT_NAME}_failure_cases.png\")\n",
    "print(f\"   \u2022 IoU distribution: {EXPERIMENT_NAME}_iou_distribution.png\")\n",
    "print(f\"   \u2022 Sample-level results: {EXPERIMENT_NAME}_sample_results.csv\")\n",
    "print(f\"   \u2022 All predictions: all_predictions/ ({len(sample_df)} images)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# CLEANUP\n",
    "# ============================================================\n",
    "\n",
    "del best_model, base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\u2705 ANALYSIS COMPLETE FOR {EXPERIMENT_NAME}\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"  - {plot_path}\")\n",
    "print(f\"  - {viz_path}\")\n",
    "print(f\"  - {sample_iou_path}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrgWK9a2gnoG"
   },
   "source": [
    "<a id=\"results\"></a>\n",
    "## Final Results and Analysis\n",
    "\n",
    "After completing all training runs with a consistent evaluation methodology (4-bit quantization, true Mean IoU, and SDPA attention), I ran a final evaluation on the test set to determine the most effective fine-tuning strategy.\n",
    "\n",
    "#### **Quantitative Comparison**\n",
    "\n",
    "| Experiment | Mean IoU | F1@0.5 | Precision@0.5 | Recall@0.5 | Best Checkpoint | Epoch |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Baseline (Zero-Shot)** | 0.590 | 0.654 | 0.661 | 0.646 | - | - |\n",
    "| **Exp 1a: LLM LoRA + Prompt Masking** | **0.771** \u2b50 | **0.893** | **0.919** | **0.869** | checkpoint-1626 | **6** |\n",
    "| **Exp 1b: LLM LoRA (No Masking)** | 0.745 | 0.870 | 0.894 | 0.846 | checkpoint-1355 | 5 |\n",
    "| **Exp 2: Vision+LLM LoRA + Masking** | 0.748 | 0.863 | 0.880 | 0.846 | checkpoint-1626 | 6 |\n",
    "\n",
    "-----\n",
    "\n",
    "#### **Key Findings and Technical Insights**\n",
    "\n",
    "**1. LoRA Fine-Tuning is Highly Effective \u2705**\n",
    "\n",
    "My primary experiment **(Exp 1a)** was the clear winner, achieving a **Mean IoU of 0.771**. This represents a **30.7% relative improvement** over the strong zero-shot baseline of 0.590 and confirms that QLoRA is an extremely effective technique for this task.\n",
    "\n",
    "**2. Prompt Masking Provides a Clear, Efficient Benefit \u2705**\n",
    "\n",
    "As hypothesized, Exp 1a (with masking) outperformed Exp 1b (without masking), improving the Mean IoU by **3.5%**. By focusing the loss on only the model's generated response, prompt masking provides a more efficient learning signal, leading to better performance.\n",
    "\n",
    "**3. Vision Encoder Tuning Shows Diminishing Returns \u26a0\ufe0f**\n",
    "\n",
    "Adding LoRA adapters to the vision encoder (Exp 2) did not improve performance over the LLM-only approach. This strongly suggests that the pre-trained Qwen2-VL vision encoder is already highly capable, and for this task, adapting the language model's reasoning is more impactful than adapting the visual feature extraction itself.\n",
    "\n",
    "**4. Critical Technical Finding: Evaluation Consistency is Key \ud83d\udd0d**\n",
    "\n",
    "Throughout this project, I confirmed several critical factors for accurate and reproducible evaluation:\n",
    "\n",
    "  * **Image Resolution Must Match Training:** Evaluating with a different resolution than was used in training caused a **41% performance drop**.\n",
    "  * **True Mean IoU:** I implemented a corrected Mean IoU calculation where every ground-truth box contributes to the score, providing a more honest evaluation.\n",
    "  * **Quantization Consistency:** All evaluations use the same 4-bit quantization as training to ensure a fair, \"apples-to-apples\" comparison that reflects a realistic deployment scenario.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **Qualitative Analysis**\n",
    "\n",
    "**IoU Distribution Comparison**\n",
    "\n",
    "The fine-tuning dramatically improved performance, shifting the IoU distribution from a broad, uncertain spread to a sharp peak of high-quality predictions.\n",
    "\n",
    "**Training Progression & Failure Analysis**\n",
    "\n",
    "All experiments showed healthy training dynamics. The validation IoU curves demonstrate that the models learned effectively, with Exp 1a showing the most consistent improvement. The failure analysis shows that the fine-tuned model is significantly more precise than the baseline.\n",
    "\n",
    "---\n",
    "#### Qualitative Analysis of Each Experiment\n",
    "\n",
    "**Baseline vs. Best Model (Exp 1a):**\n",
    "The fine-tuned model is significantly more precise, as shown by the tighter IoU distribution around a much higher mean.\n",
    "* **IoU Distribution Plots:**\n",
    "    ![Baseline IoU Distribution](images/baseline_iou_distribution.png)\n",
    "    ![Experiment 1a IoU Distribution](images/exp1a_iou_distribution.png)\n",
    "\n",
    "**Training Progression:**\n",
    "The validation IoU curves show that all experiments learned effectively, with Exp 1a continuing to improve through all 7 epochs.\n",
    "* **Training & Validation Curves (Exp 1a):**\n",
    "    ![Training curves for Experiment 1a](images/training_validation_exp1a.png)\n",
    "* **Training & Validation Curves (Exp 1a):**\n",
    "    ![Training curves for Experiment 1a](images/training_validation_exp1b.png)\n",
    "* **Training & Validation Curves (Exp 2):**\n",
    "    ![Training curves for Experiment 2](images/training_validation_exp2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## Production Deployment: Merging LoRA Adapters\n",
    "\n",
    "For production deployment with vLLM/Triton, LoRA adapters must be merged into the base model.\n",
    "\n",
    "**Benefits:**\n",
    "- \u2705 Single model artifact (easier deployment)\n",
    "- \u2705 Faster inference (no adapter overhead)\n",
    "- \u2705 Compatible with vLLM (required for vision models)\n",
    "\n",
    "**Use the deployment script:**\n",
    "```bash\n",
    "python deploy_to_vllm.py \\\n",
    "    --adapter_path qwen2-7b-nutrition-a100_exp1a/checkpoint-1626 \\\n",
    "    --output_dir qwen2-7b-nutrition-merged\n",
    "```\n",
    "\n",
    "See [`deploy_to_vllm.py`](deploy_to_vllm.py) for implementation details.\n",
    "\n",
    "---\n",
    "\n",
    "**Manual merge example (for reference):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE EVALUATION - Quick Analysis with evaluate_vlm\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Evaluates the baseline Qwen2-VL-7B model (no fine-tuning) on the test set.\n",
    "\n",
    "EVALUATION SETUP:\n",
    "- Uses 4-bit quantization (same as fine-tuned experiments)\n",
    "- SDPA attention (production-ready)\n",
    "- evaluate_vlm() for apples-to-apples comparison\n",
    "\n",
    "This ensures fair comparison: baseline and fine-tuned models evaluated\n",
    "under identical conditions (quantization, attention, test set).\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd0d BASELINE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_dir = \"qwen2-7b-nutrition-baseline\"\n",
    "os.makedirs(baseline_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Load baseline model (quantized like fine-tuned models during eval)\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udce6 Loading baseline Qwen2-VL-7B model...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "baseline_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "  \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    "  attn_implementation=\"sdpa\",\n",
    ")\n",
    "baseline_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "print(\"\u2705 Baseline model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Evaluate with evaluate_vlm\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udcca Running evaluate_vlm on test set...\")\n",
    "\n",
    "baseline_metrics = evaluate_vlm(\n",
    "  baseline_model,\n",
    "  baseline_processor,\n",
    "  dataset_test_downsized,  # Same test set as experiments\n",
    "  max_samples=None,\n",
    "  iou_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Baseline Performance:\")\n",
    "print(f\"   Mean GT IoU:  {baseline_metrics['mean_gt_iou']:.3f}\")\n",
    "print(f\"   Precision@0.5: {baseline_metrics['precision@0.50']:.3f}\")\n",
    "print(f\"   Recall@0.5:    {baseline_metrics['recall@0.50']:.3f}\")\n",
    "print(f\"   F1@0.5:        {baseline_metrics['f1@0.50']:.3f}\")\n",
    "\n",
    "# Save metrics\n",
    "\n",
    "metrics_path = os.path.join(baseline_dir, 'baseline_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "  json.dump(baseline_metrics, f, indent=2)\n",
    "print(f\"\\n\ud83d\udcbe Saved metrics to: {metrics_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Get per-sample IoUs for distribution analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udd0d Calculating per-sample IoUs...\")\n",
    "\n",
    "sample_df = get_sample_ious(baseline_model, baseline_processor, dataset_test_downsized)\n",
    "\n",
    "print(f\"\u2705 Evaluated {len(sample_df)} samples\")\n",
    "print(f\"   Mean IoU (per-sample): {sample_df['iou'].mean():.3f}\")\n",
    "print(f\"   Median IoU: {sample_df['iou'].median():.3f}\")\n",
    "\n",
    "# Save\n",
    "sample_path = os.path.join(baseline_dir, 'baseline_sample_results.csv')\n",
    "sample_df.to_csv(sample_path, index=False)\n",
    "print(f\"   Saved: {sample_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Plot IoU distribution\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udcca Creating IoU distribution plot...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(sample_df['iou'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0].axvline(sample_df['iou'].mean(), color='red', linestyle='--',\n",
    "              linewidth=2, label=f'Mean: {sample_df[\"iou\"].mean():.3f}')\n",
    "axes[0].axvline(sample_df['iou'].median(), color='green', linestyle='--',\n",
    "              linewidth=2, label=f'Median: {sample_df[\"iou\"].median():.3f}')\n",
    "axes[0].set_xlabel('IoU Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('BASELINE - IoU Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative\n",
    "sorted_ious = np.sort(sample_df['iou'])\n",
    "cumulative = np.arange(1, len(sorted_ious) + 1) / len(sorted_ious) * 100\n",
    "axes[1].plot(sorted_ious, cumulative, linewidth=2, color='coral')\n",
    "axes[1].axhline(80, color='red', linestyle='--', alpha=0.5, label='80th percentile')\n",
    "axes[1].axhline(50, color='green', linestyle='--', alpha=0.5, label='50th percentile')\n",
    "axes[1].set_xlabel('IoU Score', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Percentage', fontsize=12)\n",
    "axes[1].set_title('BASELINE - Cumulative Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "dist_path = os.path.join(baseline_dir, 'baseline_iou_distribution.png')\n",
    "plt.savefig(dist_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\u2705 Saved: {dist_path}\")\n",
    "\n",
    "# Print stats\n",
    "q1, q2, q3 = sample_df['iou'].quantile([0.25, 0.50, 0.75])\n",
    "excellent = (sample_df['iou'] >= 0.8).sum()\n",
    "good = ((sample_df['iou'] >= 0.6) & (sample_df['iou'] < 0.8)).sum()\n",
    "poor = ((sample_df['iou'] >= 0.3) & (sample_df['iou'] < 0.6)).sum()\n",
    "failures = (sample_df['iou'] < 0.3).sum()\n",
    "total = len(sample_df)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Distribution Statistics:\")\n",
    "print(f\"   Quartiles: {q1:.3f} / {q2:.3f} / {q3:.3f}\")\n",
    "print(f\"\\n   Performance Buckets:\")\n",
    "print(f\"   Excellent (\u22650.8): {excellent:3d} ({excellent/total*100:.1f}%)\")\n",
    "print(f\"   Good (0.6-0.8):   {good:3d} ({good/total*100:.1f}%)\")\n",
    "print(f\"   Poor (0.3-0.6):   {poor:3d} ({poor/total*100:.1f}%)\")\n",
    "print(f\"   Failures (<0.3):  {failures:3d} ({failures/total*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Save all predictions as marked-up images\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udcbe Saving all predictions as marked-up images...\")\n",
    "\n",
    "viz_dir = os.path.join(baseline_dir, 'all_predictions')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "  sample = dataset_test_downsized[row['sample_idx']]\n",
    "\n",
    "  image = sample[\"image\"].copy()\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  w, h = image.size\n",
    "\n",
    "  # Ground truth (green)\n",
    "  for y_min, x_min, y_max, x_max in sample[\"objects\"][\"bbox\"]:\n",
    "      draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"lime\", width=4\n",
    "      );\n",
    "\n",
    "  # Predictions (red)\n",
    "  for x_min, y_min, x_max, y_max in row['pred_boxes']:\n",
    "      draw.rectangle(\n",
    "          [(x_min * w, y_min * h), (x_max * w, y_max * h)],\n",
    "          outline=\"red\", width=4\n",
    "      );\n",
    "\n",
    "  # Save\n",
    "  fig, ax = plt.subplots(figsize=(10, 8));\n",
    "  ax.imshow(image);\n",
    "  ax.set_title(f\"IoU: {row['iou']:.3f} | Image ID: {row['image_id']}\",\n",
    "               fontsize=14, fontweight='bold');\n",
    "  ax.axis('off');\n",
    "\n",
    "  handles = [\n",
    "      plt.Line2D([0], [0], color='lime', linewidth=3, label='Ground Truth'),\n",
    "      plt.Line2D([0], [0], color='red', linewidth=3, label='Prediction')\n",
    "  ]\n",
    "  ax.legend(handles=handles, loc='upper right', fontsize=10);\n",
    "\n",
    "  filename = f\"{row['iou']:.3f}_{row['image_id']}.png\"\n",
    "  plt.savefig(os.path.join(viz_dir, filename), bbox_inches='tight', dpi=100);\n",
    "  plt.close()\n",
    "\n",
    "print(f\"\u2705 Saved {len(sample_df)} images to: {viz_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 BASELINE EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83d\udcc1 All outputs in: {baseline_dir}/\")\n",
    "print(f\"   \u2022 baseline_metrics.json\")\n",
    "print(f\"   \u2022 baseline_iou_distribution.png\")\n",
    "print(f\"   \u2022 baseline_sample_results.csv\")\n",
    "print(f\"   \u2022 all_predictions/ ({len(sample_df)} images)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Cleanup\n",
    "del baseline_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Ready to compare with fine-tuned experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}